{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"E:\\\\Coding\\\\Projects\\\\transformer\\\\Data file\"\n",
    "en_vocab_file = os.path.join(output_dir, \"en_vocab\")\n",
    "zh_vocab_file = os.path.join(output_dir, \"zh_vocab\")\n",
    "checkpoint_path = os.path.join(output_dir, \"checkpoints\")\n",
    "log_dir = os.path.join(output_dir, 'logs')\n",
    "download_dir = \"tensorflow-datasets/downloads\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{NamedSplit('train'): ['newscommentary_v14',\n",
      "                       'wikititles_v1',\n",
      "                       'uncorpus_v1',\n",
      "                       'casia2015',\n",
      "                       'casict2011',\n",
      "                       'casict2015',\n",
      "                       'datum2015',\n",
      "                       'datum2017',\n",
      "                       'neu2017'],\n",
      " NamedSplit('validation'): ['newstest2018']}\n"
     ]
    }
   ],
   "source": [
    "## reference:https://www.tensorflow.org/datasets/catalog/wmt_t2t_translate\n",
    "translate_builder = tfds.builder(\"wmt19_translate/zh-en\")\n",
    "pprint(translate_builder.subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Using custom data configuration zh-en\n"
     ]
    }
   ],
   "source": [
    "## I use sub-dataset:newscommentary\n",
    "## download the data\n",
    "## extract the data to csv. file\n",
    "## load every row in the file\n",
    "## shuffle data\n",
    "## change the data to TFR data, reference: https://www.tensorflow.org/guide/data#consuming_tfrecord_data\n",
    "\n",
    "config = tfds.translate.wmt.WmtConfig(version=tfds.core.Version('0.0.3', experiments={tfds.core.Experiment.S3: False}),\n",
    "                                      language_pair=(\"zh\", \"en\"),subsets={tfds.Split.TRAIN: [\"newscommentary_v14\"]})\n",
    "\n",
    "builder = tfds.builder(\"wmt_translate\", config=config)\n",
    "builder.download_and_prepare(download_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(NamedSplit('train')(tfds.percent[0:20]),\n",
       " NamedSplit('train')(tfds.percent[20:21]),\n",
       " NamedSplit('train')(tfds.percent[21:100]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## use 15% of original dataset as train data and 1% as val data because original dataset is too huge\n",
    "\n",
    "train_part = 20\n",
    "val_part = 1\n",
    "drop_part = 100 - train_part - val_part\n",
    "\n",
    "split = tfds.Split.TRAIN.subsplit([train_part, val_part, drop_part])\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_OptionsDataset shapes: ((), ()), types: (tf.string, tf.string)>\n",
      "<_OptionsDataset shapes: ((), ()), types: (tf.string, tf.string)>\n"
     ]
    }
   ],
   "source": [
    "## as_supervised: bool, if True, the returned tf.data.Dataset will have a 2-tuple structure (input, label). \n",
    "## If False, the default, the returned tf.data.Dataset will have a dictionary with all the features.\n",
    "\n",
    "examples = builder.as_dataset(split=split, as_supervised=True)\n",
    "train_examples, val_examples, _ = examples\n",
    "\n",
    "print(train_examples)\n",
    "print(val_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Making Do With More', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe5\\xa4\\x9a\\xe5\\x8a\\xb3\\xe5\\xba\\x94\\xe5\\xa4\\x9a\\xe5\\xbe\\x97', shape=(), dtype=string)\n",
      "----------\n",
      "tf.Tensor(b'If the Putins, Erdo\\xc4\\x9fans, and Orb\\xc3\\xa1ns of the world want to continue to benefit economically from the open international system, they cannot simply make up their own rules.', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe5\\xa6\\x82\\xe6\\x9e\\x9c\\xe6\\x99\\xae\\xe4\\xba\\xac\\xe3\\x80\\x81\\xe5\\x9f\\x83\\xe5\\xb0\\x94\\xe5\\xa4\\x9a\\xe5\\xae\\x89\\xe5\\x92\\x8c\\xe6\\xac\\xa7\\xe5\\xb0\\x94\\xe7\\x8f\\xad\\xe5\\xb8\\x8c\\xe6\\x9c\\x9b\\xe7\\xbb\\xa7\\xe7\\xbb\\xad\\xe4\\xba\\xab\\xe6\\x9c\\x89\\xe5\\xbc\\x80\\xe6\\x94\\xbe\\xe5\\x9b\\xbd\\xe9\\x99\\x85\\xe4\\xbd\\x93\\xe7\\xb3\\xbb\\xe6\\x8f\\x90\\xe4\\xbe\\x9b\\xe7\\x9a\\x84\\xe7\\xbb\\x8f\\xe6\\xb5\\x8e\\xe5\\x88\\xa9\\xe7\\x9b\\x8a\\xef\\xbc\\x8c\\xe5\\xb0\\xb1\\xe4\\xb8\\x8d\\xe8\\x83\\xbd\\xe7\\xae\\x80\\xe5\\x8d\\x95\\xe5\\x9c\\xb0\\xe5\\x88\\xb6\\xe5\\xae\\x9a\\xe8\\x87\\xaa\\xe5\\xb7\\xb1\\xe7\\x9a\\x84\\xe8\\xa7\\x84\\xe5\\x88\\x99\\xe3\\x80\\x82', shape=(), dtype=string)\n",
      "----------\n",
      "tf.Tensor(b'This ceiling can be raised only in a deep depression or other exceptional circumstances, allowing for counter-cyclical policy so long as it is agreed that the additional deficit is cyclical, rather than structural.', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\xe5\\x8f\\xaa\\xe6\\x9c\\x89\\xe5\\x9c\\xa8\\xe5\\x8f\\x91\\xe7\\x94\\x9f\\xe6\\xb7\\xb1\\xe5\\xba\\xa6\\xe8\\x90\\xa7\\xe6\\x9d\\xa1\\xe6\\x88\\x96\\xe5\\x85\\xb6\\xe4\\xbb\\x96\\xe5\\x8f\\x8d\\xe5\\xb8\\xb8\\xe4\\xba\\x8b\\xe4\\xbb\\xb6\\xe6\\x97\\xb6\\xef\\xbc\\x8c\\xe8\\xbf\\x99\\xe4\\xb8\\x80\\xe4\\xb8\\x8a\\xe9\\x99\\x90\\xe6\\x89\\x8d\\xe8\\x83\\xbd\\xe5\\x81\\x9a\\xe5\\x87\\xba\\xe8\\xb0\\x83\\xe6\\x95\\xb4\\xef\\xbc\\x8c\\xe4\\xbb\\xa5\\xe4\\xbe\\xbf\\xe8\\xae\\xa9\\xe5\\x8f\\x8d\\xe5\\x91\\xa8\\xe6\\x9c\\x9f\\xe6\\x94\\xbf\\xe7\\xad\\x96\\xe5\\xae\\x9e\\xe6\\x96\\xbd\\xe8\\xb6\\xb3\\xe5\\xa4\\x9f\\xe7\\x9a\\x84\\xe9\\x95\\xbf\\xe5\\xba\\xa6\\xef\\xbc\\x8c\\xe4\\xbd\\xbf\\xe4\\xba\\xba\\xe4\\xbb\\xac\\xe4\\xb8\\x80\\xe8\\x87\\xb4\\xe8\\xae\\xa4\\xe4\\xb8\\xba\\xe5\\xa2\\x9e\\xe5\\x8a\\xa0\\xe7\\x9a\\x84\\xe8\\xb5\\xa4\\xe5\\xad\\x97\\xe6\\x98\\xaf\\xe5\\x91\\xa8\\xe6\\x9c\\x9f\\xe6\\x80\\xa7\\xe7\\x9a\\x84\\xef\\xbc\\x8c\\xe8\\x80\\x8c\\xe4\\xb8\\x8d\\xe6\\x98\\xaf\\xe7\\xbb\\x93\\xe6\\x9e\\x84\\xe6\\x80\\xa7\\xe7\\x9a\\x84\\xe3\\x80\\x82', shape=(), dtype=string)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "## sentence in the train data are presented by unicode in two languages as tf.tensor type\n",
    "\n",
    "for en, zh in train_examples.take(3):\n",
    "    print(en)\n",
    "    print(zh)\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Making Do With More\n",
      "多劳应多得\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "If the Putins, Erdoğans, and Orbáns of the world want to continue to benefit economically from the open international system, they cannot simply make up their own rules.\n",
      "如果普京、埃尔多安和欧尔班希望继续享有开放国际体系提供的经济利益，就不能简单地制定自己的规则。\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "This ceiling can be raised only in a deep depression or other exceptional circumstances, allowing for counter-cyclical policy so long as it is agreed that the additional deficit is cyclical, rather than structural.\n",
      "只有在发生深度萧条或其他反常事件时，这一上限才能做出调整，以便让反周期政策实施足够的长度，使人们一致认为增加的赤字是周期性的，而不是结构性的。\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Fascist and communist regimes of the past, which followed a similar instrumentalist approach to democracy, come to mind here.\n",
      "在此我们想起了过去的法西斯主义和共产主义。 它们都相似地将民主作为实现其目的的工具。\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "This phase culminated with the collapse of communism in 1989, but the chance to overcome the Continent’s historical divisions now required a redefinition of the European project.\n",
      "这种状态随着1989年共产主义崩溃而达至巅峰，但是克服欧洲大陆历史性分裂的机遇现在需要重新定义欧洲计划。\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "The eurozone’s collapse (and, for all practical purposes, that of the EU itself) forces a major realignment of European politics.\n",
      "欧元区的瓦解强迫欧洲政治进行一次重大改组。\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "With energy and enthusiasm, Burden turned that operation into a thriving health (not health-care) agency that covers three cities and about 300,000 people on the western edge of Los Angeles.\n",
      "在能量与激情的推动下，波顿将BCHD打造成了欣欣向荣的健康（而非医疗）机构，其服务范围覆盖了洛杉矶西侧三座城市的30万人。\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "The result could be a world of fragmented blocs – an outcome that would undermine not only global prosperity, but also cooperation on shared challenges.\n",
      "其结果可能是一个四分五裂的世界 — — 这一结果不但会破坏全球繁荣，也会破坏面对共同挑战的合作。\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Among the questions being asked by NGOs, the UN, and national donors is how to prevent the recurrence of past mistakes.\n",
      "现在NGO们、联合国和捐助国们问得最多的一个问题就是如何避免再犯过去的错误。\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Managing the rise of NCDs will require long-term thinking, and government leaders will have to make investments that might pay off only after they are no longer in office.\n",
      "管理NCD的增加需要长期思维，政府领导人必须进行要在他们离任多年后才能收回成本的投资。\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## decode\n",
    "\n",
    "sample_examples = []\n",
    "num_samples = 10\n",
    "\n",
    "for en_t, zh_t in train_examples.take(num_samples):\n",
    "    en = en_t.numpy().decode(\"utf-8\")\n",
    "    zh = zh_t.numpy().decode(\"utf-8\")\n",
    "    \n",
    "    print(type(en_t))\n",
    "    print(en)\n",
    "    print(zh)\n",
    "    print('-' * 100)\n",
    "  \n",
    "   \n",
    "    sample_examples.append((en, zh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from corpus： E:\\Coding\\Projects\\transformer\\Data file\\en_vocab\n",
      "corpus size：8135\n",
      "first ten subwords：[', ', 'the_', 'of_', 'to_', 'and_', 's_', 'in_', 'a_', 'that_', 'is_']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    subword_encoder_en = tfds.features.text.SubwordTextEncoder.load_from_file(en_vocab_file)\n",
    "    print(f'loading data from corpus： {en_vocab_file}')\n",
    "except:\n",
    "    print('building corpus....')\n",
    "    subword_encoder_en = tfds.features.text.SubwordTextEncoder.build_from_corpus((en.numpy() for en, _ in train_examples), \n",
    "                         target_vocab_size=2**13) \n",
    "  \n",
    "\n",
    "    subword_encoder_en.save_to_file(en_vocab_file)\n",
    "  \n",
    "print(f'corpus size：{subword_encoder_en.vocab_size}')\n",
    "print(f'first ten subwords：{subword_encoder_en.subwords[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2700, 7911, 10, 2942, 7457, 1163, 7925]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_string = 'Taiwan is beautiful.'\n",
    "indices = subword_encoder_en.encode(sample_string)\n",
    "indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Taiwan is beautiful.', 'Taiwan is beautiful.')\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Taiwan is beautiful.'\n",
    "indices = subword_encoder_en.encode(sample_string)\n",
    "decoded_string = subword_encoder_en.decode(indices)\n",
    "assert decoded_string == sample_string\n",
    "pprint((sample_string, decoded_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index     Subword\n",
      "---------------\n",
      " 2700     Taiwan\n",
      " 7911      \n",
      "   10     is \n",
      " 2942     bea\n",
      " 7457     uti\n",
      " 1163     ful\n",
      " 7925     .\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:10}{1:6}\".format(\"Index\", \"Subword\"))\n",
    "print(\"-\" * 15)\n",
    "for idx in indices:\n",
    "    subword = subword_encoder_en.decode([idx])\n",
    "    print('{0:5}{1:6}'.format(idx, ' ' * 5 + subword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from corpus： E:\\Coding\\Projects\\transformer\\Data file\\zh_vocab\n",
      "corpus size：4201\n",
      "first ten subwords：['的', '，', '。', '国', '在', '是', '一', '和', '不', '这']\n"
     ]
    }
   ],
   "source": [
    "## max_subword_length=1:every Chinese character can be seen as a subword in corpus.\n",
    "\n",
    "try:\n",
    "    subword_encoder_zh = tfds.features.text.SubwordTextEncoder.load_from_file(zh_vocab_file)\n",
    "    print(f'loading data from corpus： {zh_vocab_file}')\n",
    "except:\n",
    "    print('building corpus....')\n",
    "    subword_encoder_zh = tfds.features.text.SubwordTextEncoder.build_from_corpus((zh.numpy() for _, zh in train_examples), \n",
    "                         target_vocab_size=2**13, max_subword_length=1) # a Chinese character is a subword in corpus\n",
    "    \n",
    "    subword_encoder_zh.save_to_file(zh_vocab_file)\n",
    "\n",
    "print(f'corpus size：{subword_encoder_zh.vocab_size}')\n",
    "print(f'first ten subwords：{subword_encoder_zh.subwords[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "多劳应多得\n",
      "[48, 557, 116, 48, 81]\n"
     ]
    }
   ],
   "source": [
    "sample_string = sample_examples[0][1]\n",
    "indices = subword_encoder_zh.encode(sample_string)\n",
    "print(sample_string)\n",
    "print(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add BOS and EOS to the sequence, use subword_encoder_en.vocab_size/+1 as index of BOS/EOS\n",
    "## 增加<start>及<end>字符，這邊用tokenizer_pt.vocab_szie代替因為字典大小實際上是(0,tokenizer_pt.vocab_size-1)\n",
    "\n",
    "def encode(en_t, zh_t):\n",
    "   \n",
    "    en_indices = [subword_encoder_en.vocab_size] + subword_encoder_en.encode(en_t.numpy())\\\n",
    "               + [subword_encoder_en.vocab_size + 1]\n",
    "    \n",
    "    zh_indices = [subword_encoder_zh.vocab_size] + subword_encoder_zh.encode(zh_t.numpy())\\\n",
    "               + [subword_encoder_zh.vocab_size + 1]\n",
    "  \n",
    "    return en_indices, zh_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tensor doesn't have the numpy attribute, can't directly use dataset.map(en_t,zh_t)\n",
    "## tf.py_function allows expressing computations in a TensorFlow graph as Python functions.\n",
    "\n",
    "def tf_encode(en_t, zh_t):\n",
    " \n",
    "    return tf.py_function(encode, [en_t, zh_t], [tf.int64, tf.int64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 40\n",
    "\n",
    "def filter_max_length(en, zh, max_length=max_len):\n",
    "  # tf.logical_and == (True,True), only return when en and zh are both True\n",
    "    return tf.logical_and(tf.size(en) <= max_length,tf.size(zh) <= max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check the size of the dataset after filter sentence whose length > 40 \n",
    "\n",
    "num_examples = 0\n",
    "for en_indices, zh_indices in tmp_dataset:\n",
    "    count_en = len(en_indices) <= max_len\n",
    "    count_zh = len(zh_indices) <= max_len\n",
    "    assert count_en and count_zh\n",
    "    num_examples += 1\n",
    "\n",
    "print(f'Data size:{num_examples}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\".map() 内部的操作以图模式（graph mode）运行，.map() 接收一个不具有 numpy 属性的图张量（graph tensor）。\n",
    "该分词器（tokenizer）需要将一个字符串或 Unicode 符号，编码成整数。因此，您需要在 tf.py_function 内部运行编码过程，\n",
    "tf.py_function 接收一个 eager 张量，该 eager 张量有一个包含字符串值的 numpy 属性。\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "buffer_size = 15000\n",
    "\n",
    "train_encode = train_examples.map(tf_encode)  # 輸出：(英文句子, 中文句子)\n",
    "train_filt = train_encode.filter(filter_max_length).cache()  # 輸出：(英文索引序列, 中文索引序列)\n",
    "train_shuffle = train_filt.shuffle(buffer_size)  # 將例子洗牌確保隨機性\n",
    "train_dataset = train_shuffle.padded_batch(batch_size, padded_shapes=([-1], [-1])).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_encode = train_examples.map(tf_encode)\n",
    "val_filt = train_encode.filter(filter_max_length).cache() \n",
    "val_shuffle = train_filt.shuffle(buffer_size) \n",
    "val_dataset = train_shuffle.padded_batch(batch_size, padded_shapes=([-1], [-1])).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('It is important.', '这很重要。'),\n",
      " ('The numbers speak for themselves.', '数字证明了一切。')]\n"
     ]
    }
   ],
   "source": [
    "demo_examples = [\n",
    "    (\"It is important.\", \"这很重要。\"),\n",
    "    (\"The numbers speak for themselves.\", \"数字证明了一切。\"),\n",
    "]\n",
    "pprint(demo_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8135  105   10 1304 7925 8136    0    0]\n",
      " [8135   17 3905 6013   12 2572 7925 8136]], shape=(2, 8), dtype=int64)\n",
      "\n",
      "tar: tf.Tensor(\n",
      "[[4201   10  241   80   27    3 4202    0    0    0]\n",
      " [4201  162  467  421  189   14    7  553    3 4202]], shape=(2, 10), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "demo_examples = tf.data.Dataset.from_tensor_slices((\n",
    "    [en for en, _ in demo_examples], [zh for _, zh in demo_examples]\n",
    "))\n",
    "\n",
    "# 將兩個句子透過之前定義的字典轉換成子詞的序列（sequence of subwords）\n",
    "# 並添加 padding token: <pad> 來確保 batch 裡的句子有一樣長度\n",
    "demo_dataset = demo_examples.map(tf_encode)\\\n",
    "  .padded_batch(batch_size, padded_shapes=([-1], [-1]))\n",
    "\n",
    "# 取出這個 demo dataset 裡唯一一個 batch\n",
    "inp, tar = next(iter(demo_dataset))\n",
    "print('inp:', inp)\n",
    "print('' * 10)\n",
    "print('tar:', tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## position encoding\n",
    "\n",
    "\"\"\"\n",
    "論文裡頭提到他們之所以這樣設計位置編碼（Positional Encoding, PE）是因為這個函數有個很好的特性：\n",
    "給定任一位置 pos 的位置編碼 PE(pos)，跟它距離 k 個單位的位置 pos + k 的位置編碼 PE(pos + k) \n",
    "可以表示為 PE(pos) 的一個線性函數（linear function）。\n",
    "第 1 維代表 batch_size，之後可以 broadcasting\n",
    "第 2 維是序列長度，我們會為每個在輸入 / 輸出序列裡頭的子詞都加入位置編碼\n",
    "第 3 維跟詞嵌入向量同維度\n",
    "\"\"\"\n",
    "\n",
    "# PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "# PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "# pos.shape == (sentence_len,1)\n",
    "# d_model.shape == (1,d_model)\n",
    "# d_model為word embedding的維度\n",
    "# get_angles.shape:(sentence_len, d_model)\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    \n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model)) # 不管i為奇數還是偶數經過(2*(i//2)) 後都變為偶數\n",
    "    \n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    \n",
    "    # position  == max_seq_len\n",
    "    \n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis], # np.newaxis:增加新的一個維度\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "    \n",
    "    # sin.shape == (sentence_len,d_model/2)\n",
    "    # cos.shape == (sentence_len,d_model/2)\n",
    "    \n",
    "    sin = np.sin(angle_rads[:,0::2]) #取出偶數位\n",
    "    cos = np.cos(angle_rads[:, 1::2]) #第一個冒號代表1到最後,第二個冒號代表隔一個取一個數 == 取奇數\n",
    "    \n",
    "    # pos_embedding.shape ==(sentence_len,d_model)\n",
    "    \n",
    "    pos_encoding = tf.concat([sin,cos],axis=-1)\n",
    "    \n",
    "    # pos_embedding.shape ==(1, sentence_len, d_model), 為輸入batch_size做擴展\n",
    "    \n",
    "    pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(10)[:, np.newaxis].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 100)\n"
     ]
    }
   ],
   "source": [
    "pos_demo = positional_encoding(50, 100)\n",
    "print(pos_demo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 8, 4), dtype=float32, numpy=\n",
       " array([[[ 0.04802907,  0.0417271 ,  0.02344806,  0.00834734],\n",
       "         [-0.00591327, -0.00660323, -0.01324582, -0.0348642 ],\n",
       "         [ 0.03332186,  0.03574366,  0.02287287, -0.00660708],\n",
       "         [ 0.04281593,  0.04223473, -0.00088052,  0.00522784],\n",
       "         [-0.04101226, -0.00658475, -0.00472261, -0.03033475],\n",
       "         [-0.01220929, -0.02931843, -0.02383747, -0.04168735],\n",
       "         [-0.04469776, -0.02076419, -0.00033796, -0.0356768 ],\n",
       "         [-0.04469776, -0.02076419, -0.00033796, -0.0356768 ]],\n",
       " \n",
       "        [[ 0.04802907,  0.0417271 ,  0.02344806,  0.00834734],\n",
       "         [-0.02072092,  0.0475449 , -0.0217829 , -0.04648754],\n",
       "         [-0.01421092,  0.02174482,  0.02294401,  0.03281916],\n",
       "         [ 0.04423733, -0.04595483,  0.02939636, -0.01437107],\n",
       "         [-0.04825069,  0.01481294, -0.03381839,  0.02953986],\n",
       "         [-0.00834382, -0.04570672,  0.00123869,  0.0025998 ],\n",
       "         [-0.04101226, -0.00658475, -0.00472261, -0.03033475],\n",
       "         [-0.01220929, -0.02931843, -0.02383747, -0.04168735]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 10, 4), dtype=float32, numpy=\n",
       " array([[[ 0.03757041, -0.00225263,  0.02857361, -0.04717558],\n",
       "         [ 0.04812047,  0.01244806,  0.0106625 ,  0.04305186],\n",
       "         [-0.00379892, -0.04100956,  0.00787407, -0.04874456],\n",
       "         [ 0.03287658, -0.02525207, -0.03699771,  0.00725321],\n",
       "         [ 0.02553995,  0.04599858, -0.00789443,  0.02946942],\n",
       "         [-0.02615911, -0.01936175, -0.02644104, -0.01803677],\n",
       "         [-0.02817409,  0.04613421, -0.02102358,  0.0239752 ],\n",
       "         [-0.01670324, -0.00721968,  0.04367585,  0.00826793],\n",
       "         [-0.01670324, -0.00721968,  0.04367585,  0.00826793],\n",
       "         [-0.01670324, -0.00721968,  0.04367585,  0.00826793]],\n",
       " \n",
       "        [[ 0.03757041, -0.00225263,  0.02857361, -0.04717558],\n",
       "         [ 0.01819582, -0.01850195,  0.0314086 , -0.00169832],\n",
       "         [ 0.04122499,  0.00235736,  0.04385987, -0.03879323],\n",
       "         [-0.04820785, -0.00737901,  0.01049312, -0.0189888 ],\n",
       "         [-0.04720581, -0.01043029,  0.03743596, -0.0215044 ],\n",
       "         [-0.04788531, -0.02013778, -0.01900253,  0.04804924],\n",
       "         [ 0.04205528,  0.04144123, -0.04549149, -0.01219107],\n",
       "         [ 0.0096045 ,  0.02175846, -0.01613778, -0.04110118],\n",
       "         [-0.02615911, -0.01936175, -0.02644104, -0.01803677],\n",
       "         [-0.02817409,  0.04613421, -0.02102358,  0.0239752 ]]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## + 2 是因為我們額外加了 <start> 以及 <end> tokens\n",
    "\n",
    "vocab_size_en_demo = subword_encoder_en.vocab_size + 2\n",
    "vocab_size_zh_demo = subword_encoder_zh.vocab_size + 2\n",
    "\n",
    "## 為了方便 demo, 將詞彙轉換到一個 4 維的詞嵌入空間\n",
    "\n",
    "d_model_demo = 4\n",
    "embedding_layer_en = tf.keras.layers.Embedding(vocab_size_en_demo, d_model_demo)\n",
    "embedding_layer_zh = tf.keras.layers.Embedding(vocab_size_zh_demo, d_model_demo)\n",
    "\n",
    "emb_inp = embedding_layer_en(inp)\n",
    "emb_tar = embedding_layer_zh(tar)\n",
    "emb_inp, emb_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8135  105   10 1304 7925 8136    0    0]\n",
      " [8135   17 3905 6013   12 2572 7925 8136]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "inp_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "build padding_mask: padding mask 是讓 Transformer 用來識別序列實際的內容到哪裡。\n",
    "此遮罩負責的就是將序列中被補 0 的地方（也就是 <pad>）的位置蓋住，讓 Transformer 可以避免「關注」到這些位置。\n",
    "\"\"\"\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    \n",
    "    # padding mask 的工作就是把索引序列中為 0 的位置設為 1\n",
    "\n",
    "    mask = tf.cast(tf.equal(seq, 0), tf.float32)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :] #　broadcasting\n",
    "\n",
    "print(\"inp:\", inp)\n",
    "inp_mask = create_padding_mask(inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"inp_mask:\", inp_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scaled_dot_product_attention\n",
    "\n",
    "\"\"\"\n",
    "Calculate the attention weights.\n",
    "q, k, v must have matching leading dimensions.\n",
    "k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "The mask has different shapes depending on its type(padding or look ahead) \n",
    "but it must be broadcastable for addition.\n",
    "  \n",
    "Args:\n",
    "q: query shape == (..., seq_len_q, depth)\n",
    "k: key shape == (..., seq_len_k, depth)\n",
    "v: value shape == (..., seq_len_v, depth_v)\n",
    "mask: Float tensor with shape broadcastable \n",
    "to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "Returns:\n",
    "output ( 將 matmul_qk 除以 scaling factor sqrt(dk)在座加權平均), attention_weights\n",
    "\n",
    "Scaled dot product attention 跟以往 multiplicative attention 一樣是先將維度相同的 Q 跟 K 做點積：\n",
    "將對應維度的值兩兩相乘後相加得到單一數值，接著把這些數值除以一個 scaling factor sqrt(dk) ，\n",
    "然後再丟入 softmax 函式得到相加為 1 的注意權重（attention weights）。\n",
    "\n",
    "output:代表注意力機制的結果,此張量也被稱作「注意張量」，\n",
    "你可以將其解讀為 q 在關注 k 並從 v 得到上下文訊息後的所獲得的新 representation。\n",
    "attention_weights:代表句子 q 裡頭每個子詞對句子 k 裡頭的每個子詞的注意權重,\n",
    "而注意權重 attention_weights 則是 q 裡頭每個句子的每個子詞對其他位置的子詞的關注程度。\n",
    "\n",
    "再次提醒，因為我們輸入注意函式的 q 跟 k 都是同樣的英文詞嵌入張量 emb_inp，\n",
    "事實上這邊做的就是讓英文句子裡頭的每個子詞都去關注同句子中其他位置的子詞的資訊，並從中獲得上下文語義，\n",
    "而這就是所謂的自注意力機制（self-attention）：序列關注自己。\n",
    "當序列 q 換成 Decoder 的輸出序列而序列 k 變成 Encoder 的輸出序列時，\n",
    "我們就變成在計算一般 Seq2Seq 模型中的注意力機制。這點觀察非常重要，且我們在前面就已經提過了。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "   \n",
    "    # 將 `q`、 `k` 做點積再 scale\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)  # 取得 seq_k 的序列長度\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)  # scale by sqrt(dk)\n",
    "\n",
    "    # 將遮罩「加」到被丟入 softmax 前的 logits\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9) # * -1e9 使得做完softmax後value接近0, 達到padding mask的效果\n",
    "\n",
    "    # 取 softmax 是為了得到總和為 1 的比例之後對 `v` 做加權平均\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "    # 以注意權重對 v 做加權平均（weighted average）\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tf.Tensor(\n",
      "[[[0.49982947 0.625106   0.749951   0.37458116]\n",
      "  [0.5000564  0.62498987 0.75003207 0.3751459 ]\n",
      "  [0.4998769  0.62507844 0.7499711  0.37470973]\n",
      "  [0.4998746  0.62508476 0.74995136 0.37464327]\n",
      "  [0.50012285 0.6249185  0.7500388  0.37530604]\n",
      "  [0.5001022  0.6249755  0.7500555  0.37527394]\n",
      "  [0.5001374  0.6249151  0.75005543 0.3753738 ]\n",
      "  [0.5001374  0.6249151  0.75005543 0.3753738 ]]\n",
      "\n",
      " [[0.4996341  0.3749673  0.5003067  0.3750055 ]\n",
      "  [0.50048715 0.37517703 0.49988955 0.37506312]\n",
      "  [0.49988538 0.37485886 0.4999286  0.3750843 ]\n",
      "  [0.49956125 0.37509084 0.5003358  0.37486422]\n",
      "  [0.5003757  0.37485057 0.4996159  0.3751034 ]\n",
      "  [0.4999561  0.37500054 0.49995735 0.37494534]\n",
      "  [0.5003707  0.3751251  0.49979895 0.37502754]\n",
      "  [0.5002974  0.37517726 0.49993348 0.37493664]]], shape=(2, 8, 4), dtype=float32)\n",
      "output shape: (2, 8, 4)\n",
      "--------------------\n",
      "attention_weights: tf.Tensor(\n",
      "[[[0.12530005 0.12493541 0.1252315  0.12524833 0.12484504 0.12483828\n",
      "   0.1248007  0.1248007 ]\n",
      "  [0.12488184 0.12504621 0.12492277 0.12491047 0.12504224 0.12508157\n",
      "   0.12505747 0.12505747]\n",
      "  [0.12521042 0.12495533 0.12517172 0.12516715 0.12489263 0.1248792\n",
      "   0.12486178 0.12486178]\n",
      "  [0.12524362 0.12495936 0.12518351 0.12523131 0.12486656 0.12488095\n",
      "   0.12481734 0.12481734]\n",
      "  [0.12479062 0.12504137 0.1248592  0.12481682 0.12512027 0.12508292\n",
      "   0.1251444  0.1251444 ]\n",
      "  [0.12478093 0.12507775 0.12484284 0.12482828 0.12507999 0.1251578\n",
      "   0.1251162  0.1251162 ]\n",
      "  [0.12474225 0.12505254 0.12482432 0.12476357 0.12514035 0.12511508\n",
      "   0.12518093 0.12518093]\n",
      "  [0.12474225 0.12505254 0.12482432 0.12476357 0.12514035 0.12511508\n",
      "   0.12518093 0.12518093]]\n",
      "\n",
      " [[0.12531747 0.12503098 0.12509018 0.12507391 0.12488505 0.12488434\n",
      "   0.1248624  0.12485565]\n",
      "  [0.12497483 0.1253024  0.12492565 0.12477729 0.12503593 0.12483506\n",
      "   0.12509735 0.12505147]\n",
      "  [0.12507582 0.12496743 0.12515348 0.12492194 0.12508611 0.1249634\n",
      "   0.12496947 0.12486237]\n",
      "  [0.12506089 0.12482036 0.1249233  0.12533401 0.12474798 0.12512055\n",
      "   0.12493645 0.12505646]\n",
      "  [0.12485191 0.12505892 0.1250673  0.12472787 0.12527773 0.12497722\n",
      "   0.12506376 0.12497529]\n",
      "  [0.12485622 0.12486303 0.12494962 0.1251054  0.12498223 0.12513271\n",
      "   0.12503211 0.12507874]\n",
      "  [0.12480579 0.12509683 0.12492716 0.1248928  0.12504022 0.12500356\n",
      "   0.12513548 0.12509814]\n",
      "  [0.12480416 0.12505609 0.12482523 0.1250179  0.12495691 0.12505531\n",
      "   0.12510328 0.12518111]]], shape=(2, 8, 8), dtype=float32)\n",
      "attention_weights shape: (2, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "## scaled_dot_product_attention demo\n",
    "\n",
    "# 自注意力機制：查詢 `q` 跟鍵值 `k` 都是 `emb_inp`\n",
    "q = emb_inp\n",
    "k = emb_inp\n",
    "# 簡單產生一個跟 `emb_inp` 同樣 shape 的 binary vector\n",
    "v = tf.cast(tf.math.greater(tf.random.uniform(shape=emb_inp.shape), 0.5), tf.float32)\n",
    "\n",
    "mask = None\n",
    "output, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "print(\"output:\", output)\n",
    "print('output shape:',output.shape)\n",
    "print(\"-\" * 20)\n",
    "print(\"attention_weights:\", attention_weights)\n",
    "print('attention_weights shape:',attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "if attention weights ==(3,3) \n",
    "eg: [[1,2,3]\n",
    "     [2,3,4]\n",
    "     [3,4,6]]\n",
    "\n",
    "(row=0, col=0) : 第一個word對第一個word的關注程度\n",
    "(row=0, col=1) : 第一個word對第二個word的關注程度\n",
    "(row=1, col=0) : 第二個word對第二個word的關注程度\n",
    "(row=2, col=1) : 第三個word對第二個word的關注程度\n",
    "\n",
    "若要讓前面的單字關注不到後面的單字則須使矩陣變成:\n",
    "    [[1,0,0]\n",
    "    [2,3,0]\n",
    "    [3,4,6]]\n",
    "    \n",
    "只有下方三角區域有值，上方三角區域為0,即被mask掉\n",
    "\"\"\"\n",
    "\n",
    "# 建立一個 2 維矩陣，維度為 (size, size)，\n",
    "# 其遮罩為一個右上角的三角形\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 0.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## matrix demo\n",
    "\n",
    "#標記出上三角為0的部分\n",
    "\n",
    "print(1-tf.linalg.band_part(tf.ones((3, 3)), -1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_tar: tf.Tensor(\n",
      "[[[ 0.03757041 -0.00225263  0.02857361 -0.04717558]\n",
      "  [ 0.04812047  0.01244806  0.0106625   0.04305186]\n",
      "  [-0.00379892 -0.04100956  0.00787407 -0.04874456]\n",
      "  [ 0.03287658 -0.02525207 -0.03699771  0.00725321]\n",
      "  [ 0.02553995  0.04599858 -0.00789443  0.02946942]\n",
      "  [-0.02615911 -0.01936175 -0.02644104 -0.01803677]\n",
      "  [-0.02817409  0.04613421 -0.02102358  0.0239752 ]\n",
      "  [-0.01670324 -0.00721968  0.04367585  0.00826793]\n",
      "  [-0.01670324 -0.00721968  0.04367585  0.00826793]\n",
      "  [-0.01670324 -0.00721968  0.04367585  0.00826793]]\n",
      "\n",
      " [[ 0.03757041 -0.00225263  0.02857361 -0.04717558]\n",
      "  [ 0.01819582 -0.01850195  0.0314086  -0.00169832]\n",
      "  [ 0.04122499  0.00235736  0.04385987 -0.03879323]\n",
      "  [-0.04820785 -0.00737901  0.01049312 -0.0189888 ]\n",
      "  [-0.04720581 -0.01043029  0.03743596 -0.0215044 ]\n",
      "  [-0.04788531 -0.02013778 -0.01900253  0.04804924]\n",
      "  [ 0.04205528  0.04144123 -0.04549149 -0.01219107]\n",
      "  [ 0.0096045   0.02175846 -0.01613778 -0.04110118]\n",
      "  [-0.02615911 -0.01936175 -0.02644104 -0.01803677]\n",
      "  [-0.02817409  0.04613421 -0.02102358  0.0239752 ]]], shape=(2, 10, 4), dtype=float32)\n",
      "--------------------\n",
      "look_ahead_mask tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## look_ahead_mask demo\n",
    "\n",
    "seq_len = emb_tar.shape[1] # 注意這次我們用中文的詞嵌入張量 `emb_tar`\n",
    "look_ahead_mask_demo = create_look_ahead_mask(seq_len)\n",
    "print(\"emb_tar:\", emb_tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"look_ahead_mask\", look_ahead_mask_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'look_ahead_mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-ac87b4b79751>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# 將 look_ahead_mask 放入注意函式\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m _, attention_weights_demo = scaled_dot_product_attention(\n\u001b[1;32m----> 7\u001b[1;33m     temp_q, temp_k, temp_v, look_ahead_mask)\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"attention_weights:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_weights_demo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'look_ahead_mask' is not defined"
     ]
    }
   ],
   "source": [
    "temp_q = temp_k = emb_tar\n",
    "temp_v = tf.cast(tf.math.greater(\n",
    "    tf.random.uniform(shape=emb_tar.shape), 0.5), tf.float32)\n",
    "\n",
    "# 將 look_ahead_mask 放入注意函式\n",
    "_, attention_weights_demo = scaled_dot_product_attention(\n",
    "    temp_q, temp_k, temp_v, look_ahead_mask)\n",
    "\n",
    "print(\"attention_weights:\", attention_weights_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tf.Tensor(\n",
      "[[[ 0.04802907  0.0417271   0.02344806  0.00834734]\n",
      "  [-0.00591327 -0.00660323 -0.01324582 -0.0348642 ]\n",
      "  [ 0.03332186  0.03574366  0.02287287 -0.00660708]\n",
      "  [ 0.04281593  0.04223473 -0.00088052  0.00522784]\n",
      "  [-0.04101226 -0.00658475 -0.00472261 -0.03033475]\n",
      "  [-0.01220929 -0.02931843 -0.02383747 -0.04168735]\n",
      "  [-0.04469776 -0.02076419 -0.00033796 -0.0356768 ]\n",
      "  [-0.04469776 -0.02076419 -0.00033796 -0.0356768 ]]\n",
      "\n",
      " [[ 0.04802907  0.0417271   0.02344806  0.00834734]\n",
      "  [-0.02072092  0.0475449  -0.0217829  -0.04648754]\n",
      "  [-0.01421092  0.02174482  0.02294401  0.03281916]\n",
      "  [ 0.04423733 -0.04595483  0.02939636 -0.01437107]\n",
      "  [-0.04825069  0.01481294 -0.03381839  0.02953986]\n",
      "  [-0.00834382 -0.04570672  0.00123869  0.0025998 ]\n",
      "  [-0.04101226 -0.00658475 -0.00472261 -0.03033475]\n",
      "  [-0.01220929 -0.02931843 -0.02383747 -0.04168735]]], shape=(2, 8, 4), dtype=float32)\n",
      "output: tf.Tensor(\n",
      "[[[[ 0.04802907  0.0417271 ]\n",
      "   [-0.00591327 -0.00660323]\n",
      "   [ 0.03332186  0.03574366]\n",
      "   [ 0.04281593  0.04223473]\n",
      "   [-0.04101226 -0.00658475]\n",
      "   [-0.01220929 -0.02931843]\n",
      "   [-0.04469776 -0.02076419]\n",
      "   [-0.04469776 -0.02076419]]\n",
      "\n",
      "  [[ 0.02344806  0.00834734]\n",
      "   [-0.01324582 -0.0348642 ]\n",
      "   [ 0.02287287 -0.00660708]\n",
      "   [-0.00088052  0.00522784]\n",
      "   [-0.00472261 -0.03033475]\n",
      "   [-0.02383747 -0.04168735]\n",
      "   [-0.00033796 -0.0356768 ]\n",
      "   [-0.00033796 -0.0356768 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.04802907  0.0417271 ]\n",
      "   [-0.02072092  0.0475449 ]\n",
      "   [-0.01421092  0.02174482]\n",
      "   [ 0.04423733 -0.04595483]\n",
      "   [-0.04825069  0.01481294]\n",
      "   [-0.00834382 -0.04570672]\n",
      "   [-0.04101226 -0.00658475]\n",
      "   [-0.01220929 -0.02931843]]\n",
      "\n",
      "  [[ 0.02344806  0.00834734]\n",
      "   [-0.0217829  -0.04648754]\n",
      "   [ 0.02294401  0.03281916]\n",
      "   [ 0.02939636 -0.01437107]\n",
      "   [-0.03381839  0.02953986]\n",
      "   [ 0.00123869  0.0025998 ]\n",
      "   [-0.00472261 -0.03033475]\n",
      "   [-0.02383747 -0.04168735]]]], shape=(2, 2, 8, 2), dtype=float32)\n",
      "(2, 2, 8, 2)\n"
     ]
    }
   ],
   "source": [
    "def split_heads(x, d_model, num_heads):\n",
    "    # x.shape: (batch_size, seq_len, d_model)\n",
    "    batch_size = tf.shape(x)[0]\n",
    "  \n",
    "    # 我們要確保維度 `d_model` 可以被平分成 `num_heads` 個 `depth` 維度\n",
    "    assert d_model % num_heads == 0\n",
    "    depth = d_model // num_heads  # 這是分成多頭以後每個向量的維度 \n",
    "  \n",
    "    # 將最後一個 d_model 維度分成 num_heads 個 depth 維度。\n",
    "    # 最後一個維度變成兩個維度，張量 x 從 3 維到 4 維\n",
    "    # (batch_size, seq_len, num_heads, depth)\n",
    "    reshaped_x = tf.reshape(x, shape=(batch_size, -1, num_heads, depth))\n",
    "  \n",
    "    # 將 head 的維度拉前使得最後兩個維度為子詞以及其對應的 depth 向量\n",
    "    # (batch_size, num_heads, seq_len, depth)\n",
    "    output = tf.transpose(reshaped_x, perm=[0, 2, 1, 3])\n",
    "  \n",
    "    return output\n",
    "\n",
    "# 我們的 `emb_inp` 裡頭的子詞本來就是 4 維的詞嵌入向量\n",
    "\n",
    "d_model_demo = 4\n",
    "\n",
    "# 將 4 維詞嵌入向量分為 2 個 head 的 2 維矩陣\n",
    "\n",
    "num_heads_demo = 2\n",
    "x = emb_inp\n",
    "\n",
    "output_demo = split_heads(x, d_model_demo, num_heads_demo)  \n",
    "print(\"x:\", x)\n",
    "print(\"output:\", output_demo)\n",
    "print(output_demo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build multiheadAttention\n",
    "\n",
    "# 實作一個執行多頭注意力機制的 keras layer\n",
    "# 在初始的時候指定輸出維度 `d_model` & `num_heads，\n",
    "# 在呼叫的時候輸入 `v`, `k`, `q` 以及 `mask`\n",
    "# 輸出跟 scaled_dot_product_attention 函式一樣有兩個：\n",
    "# output.shape            == (batch_size, seq_len_q, d_model)\n",
    "# attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "\"\"\"\n",
    "複習完了嗎？mutli-head attention 的概念本身並不難，用比較正式的說法就是將 Q、K 以及 V 這三個張量先個別轉換到 d_model 維空間，\n",
    "再將其拆成多個比較低維的 depth 維度 N(num_heads) 次以後，將這些產生的小 q、小 k 以及小 v 分別丟入前面的注意函式得到 N 個結果。\n",
    "接著將這 N 個 heads 的結果串接起來，最後通過一個線性轉換就能得到 multi-head attention 的輸出\n",
    "\n",
    "q->Wq->Q->split->q0,q1,q2\n",
    "k,v依此類推\n",
    "\"\"\"\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads # 指定要將 `d_model` 拆成幾個 heads\n",
    "        self.d_model = d_model # 在 split_heads 之前的基底維度\n",
    "        \n",
    "        # 我們要確保維度 `d_model` 可以被平分成 `num_heads` 個 `depth` 維度\n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // self.num_heads # 每個 head 裡子詞的新的 repr. 維度\n",
    "        \n",
    "        # 分別給 q, k, v 的 3 個線性轉換 \n",
    "        \n",
    "        self.Wq = tf.keras.layers.Dense(d_model) # 經過全連接層之前, q,k,v的第二個維度為embedding_units, 經過之後便為d_model\n",
    "        self.Wk = tf.keras.layers.Dense(d_model)\n",
    "        self.Wv = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        # 多 heads 串接後通過的線性轉換\n",
    "            \n",
    "        self.dense = tf.keras.layers.Dense(d_model) # 最後通過一個線性轉換就能得到 multi-head attention 的輸出\n",
    "        \n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "    \n",
    "        # original x.shape == (batch_size, senquence_len, d_model)\n",
    "        # d_model = num_heads * depth\n",
    "        # 希望 x ->(batch_size, num_heads, seq_len, depth)\n",
    "        # 分拆最后一个维度到 (num_heads, depth)\n",
    "        # 將最後一個 d_model 維度分成 num_heads 個 depth 維度。\n",
    "        # 最後一個維度變成兩個維度，張量 x 從 3 維到 4 維\n",
    "        \n",
    "        # (batch_size, seq_len, num_heads, depth)\n",
    "        reshaped_x = tf.reshape(x, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        \n",
    "        # 转置结果使得形状为 (batch_size, num_heads, seq_len, depth), 因為在計算scaled_dot_attention時是用seq_len and depth計算\n",
    "        \n",
    "        split_output = tf.transpose(reshaped_x, perm=[0, 2, 1, 3]) # perm重新排列維度的順序\n",
    "        \n",
    "        return split_output\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        \n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        # 將輸入的 q, k, v 都各自做一次線性轉換到 `d_model` 維空間\n",
    "        \n",
    "        q = self.Wq(q)  # (batch_size, seq_len, d_model) \n",
    "        k = self.Wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.Wv(v)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # 前面看過的，將最後一個 `d_model` 維度分成 `num_heads` 個 `depth` 維度\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "        # scaled_attention_output.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        \n",
    "        # 讓q,k,v 經過scaled_attention_layer得到attention 結果\n",
    "        # seq_len_q 必須  == seq_len_v\n",
    "        \n",
    "        scaled_attention_output, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        # 跟我們在 `split_heads` 函式做的事情剛好相反，先做 transpose 再做 reshape\n",
    "        # 將 `num_heads` 個 `depth` 維度串接回原來的 `d_model` 維度\n",
    "        \n",
    "        scaled_attention = tf.transpose(scaled_attention_output, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "        \n",
    "        # 用reshape方法進行維度合併\n",
    "        \n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        # now the shape is (batch_size, seq_len_q, d_model)\n",
    "        # 通過最後一個線性轉換\n",
    "        \n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_model: 4\n",
      "num_heads: 2\n",
      "\n",
      "q.shape: (2, 8, 4)\n",
      "k.shape: (2, 8, 4)\n",
      "v.shape: (2, 8, 4)\n",
      "padding_mask.shape: (2, 1, 1, 8)\n",
      "output.shape: (2, 8, 4)\n",
      "attention_weights.shape: (2, 2, 8, 8)\n",
      "\n",
      "output: tf.Tensor(\n",
      "[[[-0.00229525  0.01271551  0.01086848  0.00304159]\n",
      "  [-0.00228559  0.01269655  0.01084968  0.00302937]\n",
      "  [-0.00229362  0.01271478  0.01086738  0.00303852]\n",
      "  [-0.00229363  0.01270943  0.0108627   0.00304021]\n",
      "  [-0.00228783  0.01270656  0.01085759  0.00302735]\n",
      "  [-0.00228216  0.01268727  0.01084116  0.00302674]\n",
      "  [-0.00228627  0.01270372  0.010855    0.00302591]\n",
      "  [-0.00228627  0.01270372  0.010855    0.00302591]]\n",
      "\n",
      " [[-0.00195317  0.00743028  0.00723878  0.00097643]\n",
      "  [-0.00206604  0.00751877  0.00729632  0.00101206]\n",
      "  [-0.00202391  0.00749027  0.00727341  0.00099713]\n",
      "  [-0.00196889  0.0074325   0.00724417  0.0009842 ]\n",
      "  [-0.00211679  0.00755834  0.00731703  0.00102903]\n",
      "  [-0.00205224  0.00750008  0.0072845   0.00101027]\n",
      "  [-0.00208253  0.00752992  0.00730292  0.00101743]\n",
      "  [-0.00207568  0.00751617  0.00729734  0.00101849]]], shape=(2, 8, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## multiHeadAttention demo\n",
    "\n",
    "# emb_inp.shape == (batch_size, seq_len, d_model)\n",
    "#               == (2, 8, 4)\n",
    "assert d_model_demo == emb_inp.shape[-1]  == 4\n",
    "num_heads_demo = 2\n",
    "\n",
    "print(f\"d_model: {d_model_demo}\")\n",
    "print(f\"num_heads: {num_heads_demo}\\n\")\n",
    "\n",
    "# 初始化一個 multi-head attention layer\n",
    "mha_demo = MultiHeadAttention(d_model_demo, num_heads_demo)\n",
    "\n",
    "# 簡單將 v, k, q 都設置為 `emb_inp`\n",
    "# 順便看看 padding mask 的作用。\n",
    "# 別忘記，第一個英文序列的最後兩個 tokens 是 <pad>\n",
    "\n",
    "v = k = q = emb_inp\n",
    "padding_mask_demo = create_padding_mask(inp)\n",
    "print(\"q.shape:\", q.shape)\n",
    "print(\"k.shape:\", k.shape)\n",
    "print(\"v.shape:\", v.shape)\n",
    "print(\"padding_mask.shape:\", padding_mask_demo.shape)\n",
    "\n",
    "output_demo, attention_weights_demo = mha_demo(v, k, q, mask)\n",
    "print(\"output.shape:\", output_demo.shape)\n",
    "print(\"attention_weights.shape:\", attention_weights_demo.shape)\n",
    "\n",
    "print(\"\\noutput:\", output_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTransformer layers結構:\\n\\nEncoder\\n輸入 Embedding\\n位置 Encoding\\nN 個 Encoder layers\\nsub-layer 1: Encoder 自注意力機制\\nsub-layer 2: Feed Forward\\n\\nDecoder\\n輸出 Embedding\\n位置 Encoding\\nN 個 Decoder layers\\nsub-layer 1: Decoder 自注意力機制\\nsub-layer 2: Decoder-Encoder 注意力機制\\nsub-layer 3: Feed Forward\\nFinal Dense Layer\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Transformer layers結構:\n",
    "\n",
    "Encoder\n",
    "輸入 Embedding\n",
    "位置 Encoding\n",
    "N 個 Encoder layers\n",
    "sub-layer 1: Encoder 自注意力機制\n",
    "sub-layer 2: Feed Forward\n",
    "\n",
    "Decoder\n",
    "輸出 Embedding\n",
    "位置 Encoding\n",
    "N 個 Decoder layers\n",
    "sub-layer 1: Decoder 自注意力機制\n",
    "sub-layer 2: Decoder-Encoder 注意力機制\n",
    "sub-layer 3: Feed Forward\n",
    "Final Dense Layer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 建立 Transformer 裡 Encoder / Decoder layer 都有使用到的 Feed Forward 元件\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  \n",
    "    # 此 FFN 對輸入做兩個線性轉換，中間加了一個 ReLU activation func\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (64, 10, 512)\n",
      "out.shape: (64, 10, 512)\n"
     ]
    }
   ],
   "source": [
    "batch_size_demo = 64\n",
    "seq_len_demo = 10\n",
    "d_model_demo = 512\n",
    "dff_demo = 2048\n",
    "\n",
    "x = tf.random.uniform((batch_size_demo, seq_len_demo, d_model_demo))\n",
    "ffn = point_wise_feed_forward_network(d_model_demo, dff_demo)\n",
    "output_demo = ffn(x)\n",
    "print(\"x.shape:\", x.shape)\n",
    "print(\"out.shape:\", output_demo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build encoder layer\n",
    "\n",
    "# Encoder 裡頭會有 N 個 EncoderLayers，而每個 EncoderLayer 裡又有兩個 sub-layers: MHA & FFN\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "x->self attention->add & normalize  & dropout\n",
    " ->feed forward->add & normalize & dropout\n",
    " \n",
    "在 Add & Norm 步驟裡頭，每個 sub-layer 會有一個殘差連結（residual connection）來幫助減緩梯度消失（Gradient Vanishing）的問題。\n",
    "接著兩個 sub-layers 都會針對最後一維 d_model 做 layer normalization，將 batch 裡頭每個子詞的輸出獨立做轉換，\n",
    "使其平均與標準差分別靠近 0 和 1 之後輸出。另外在將 sub-layer 的輸出與其輸入相加之前，我們還會做點 regularization，\n",
    "對該 sub-layer 的輸出使用 dropout。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    # Transformer 論文內預設 dropout rate 為 0.1\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        # layer norm 很常在 RNN-based 的模型被使用。一個 sub-layer 一個 layer norm\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "        # 一樣，一個 sub-layer 一個 dropout layer\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    # 需要丟入 `training` 參數是因為 dropout 在訓練以及測試的行為有所不同,測試時input不需要被dropout\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        \n",
    "        # x.shape == (batch_Size,seq_len, embedding_units)\n",
    "        # attn.shape == (batch_size, seq_len,d_model)\n",
    "        # output1.shape == (batch_size, seq_len,d_model)\n",
    "        \n",
    "        # sub-layer 1: MHA\n",
    "        # Encoder 利用注意機制關注自己當前的序列，因此 v, k, q 全部都是自己\n",
    "        # 另外別忘了我們還需要 padding mask 來遮住輸入序列中的 <pad> token\n",
    "        \n",
    "        attn_output, attn = self.mha(x, x, x, mask)  #self_attention\n",
    "        attn_output = self.dropout1(attn_output, training=training) # dropout \n",
    "        out1 = self.layernorm1(x + attn_output)  # add & norm, 且embedding_units == d_model\n",
    "    \n",
    "        # sub-layer 2: FFN\n",
    "        # ffm_output.shape == (batch_size, seq_len,d_model)\n",
    "        # output2.shape == (batch_size, seq_len,d_model)\n",
    "        \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training)  # 記得 training\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: tf.Tensor(\n",
      "[[8135  105   10 1304 7925 8136    0    0]\n",
      " [8135   17 3905 6013   12 2572 7925 8136]], shape=(2, 8), dtype=int64)\n",
      "--------------------\n",
      "padding_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32)\n",
      "--------------------\n",
      "emb_inp: tf.Tensor(\n",
      "[[[ 0.04802907  0.0417271   0.02344806  0.00834734]\n",
      "  [-0.00591327 -0.00660323 -0.01324582 -0.0348642 ]\n",
      "  [ 0.03332186  0.03574366  0.02287287 -0.00660708]\n",
      "  [ 0.04281593  0.04223473 -0.00088052  0.00522784]\n",
      "  [-0.04101226 -0.00658475 -0.00472261 -0.03033475]\n",
      "  [-0.01220929 -0.02931843 -0.02383747 -0.04168735]\n",
      "  [-0.04469776 -0.02076419 -0.00033796 -0.0356768 ]\n",
      "  [-0.04469776 -0.02076419 -0.00033796 -0.0356768 ]]\n",
      "\n",
      " [[ 0.04802907  0.0417271   0.02344806  0.00834734]\n",
      "  [-0.02072092  0.0475449  -0.0217829  -0.04648754]\n",
      "  [-0.01421092  0.02174482  0.02294401  0.03281916]\n",
      "  [ 0.04423733 -0.04595483  0.02939636 -0.01437107]\n",
      "  [-0.04825069  0.01481294 -0.03381839  0.02953986]\n",
      "  [-0.00834382 -0.04570672  0.00123869  0.0025998 ]\n",
      "  [-0.04101226 -0.00658475 -0.00472261 -0.03033475]\n",
      "  [-0.01220929 -0.02931843 -0.02383747 -0.04168735]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-1.6643548   0.23726971  0.9922278   0.43485725]\n",
      "  [-1.6423285   0.15525073  1.0376055   0.44947207]\n",
      "  [-1.6730585   0.23806566  0.9568629   0.4781298 ]\n",
      "  [-0.8472989   1.6996098  -0.5489949  -0.3033159 ]\n",
      "  [-1.7271917   0.5071882   0.5223632   0.69764036]\n",
      "  [-1.281646   -0.36840597  1.4808921   0.16916007]\n",
      "  [-1.6862413   0.21929541  0.8610107   0.60593516]\n",
      "  [-1.6862413   0.21929541  0.8610107   0.60593516]]\n",
      "\n",
      " [[ 0.2804638   1.4288095  -0.3969432  -1.3123301 ]\n",
      "  [-1.19242     1.510546   -0.5096196   0.19149363]\n",
      "  [-1.6287227   0.40994748  0.14221759  1.0765576 ]\n",
      "  [ 0.35068548 -1.3377569   1.3880379  -0.4009665 ]\n",
      "  [-1.3767405   1.0267313  -0.52825665  0.8782658 ]\n",
      "  [-0.07331455 -1.6067501   0.86889786  0.8111668 ]\n",
      "  [-1.6858472   0.775565    0.18858123  0.72170097]\n",
      "  [-0.19478399 -1.0493888   1.6435682  -0.3993953 ]]], shape=(2, 8, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## encoder layer demo\n",
    "\n",
    "d_model_demo = 4\n",
    "num_heads_demo = 2\n",
    "dff_demo = 8\n",
    "\n",
    "# 新建一個使用上述參數的 Encoder Layer\n",
    "\n",
    "enc_layer_demo = EncoderLayer(d_model_demo, num_heads_demo, dff_demo)\n",
    "padding_mask_demo = create_padding_mask(inp)  # 建立一個當前輸入 batch 使用的 padding mask\n",
    "enc_out_demo = enc_layer_demo(emb_inp, training=False, mask=padding_mask_demo)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"inp:\", inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"padding_mask:\", padding_mask_demo)\n",
    "print(\"-\" * 20)\n",
    "print(\"emb_inp:\", emb_inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out_demo)\n",
    "assert emb_inp.shape == enc_out_demo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build decoder layer\n",
    "\n",
    "\"\"\"\n",
    "Decoder layer 用 MHA 1 來關注輸出序列，查詢 Q、鍵值 K 以及值 V 都是自己。\n",
    "而之所以有個 masked 是因為（中文）輸出序列除了跟（英文）輸入序列一樣需要 padding mask 以外，\n",
    "還需要 look ahead mask 來避免 Decoder layer 關注到未來的子詞。\n",
    "在做 Masked MHA（MHA 1）的時候我們需要同時套用兩種遮罩：輸出序列的 padding mask 以及 look ahead mask。\n",
    "因此 Decoder layer 預期的遮罩是兩者結合的 combined_mask\n",
    "MHA 1 因為是 Decoder layer 關注自己，multi-head attention 的參數 v、k 以及 q 都是 x\n",
    "MHA 2 是 Decoder layer 關注 Encoder 輸出序列，因此，multi-head attention 的參數 v、k 為 enc_output，\n",
    "q 則為 MHA 1 sub-layer 的結果 out1\n",
    "\n",
    "x->self attention->add & nomalize & dropout ->out1\n",
    " out1, encoding_output-> attention -> add & nomalize & dropout ->out2\n",
    " out2 ->ffn ->add & nomalize & dropout->out3\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "# Decoder 裡頭會有 N 個 DecoderLayer，\n",
    "# 而 DecoderLayer 又有三個 sub-layers: 自注意的 MHA, 關注 Encoder 輸出的 MHA , & FFN\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        # 3 個 sub-layers 的主角們\n",
    "        \n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads) # encoder & decoder之間做attention\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "        # 定義每個 sub-layer 用的 LayerNorm\n",
    "    \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "        # 定義每個 sub-layer 用的 Dropout\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           combined_mask, inp_padding_mask):\n",
    "        \n",
    "        # x.shape == (batch_size,seq_len, d_model)\n",
    "        # enc_output 為 Encoder 輸出序列，shape 為 (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        # 所有 sub-layers 的主要輸出皆為 (batch_size, target_seq_len, d_model)\n",
    "        \n",
    "        # attn_weights1 則為 (batch_size, num_heads, target_seq_len, target_seq_len)\n",
    "        # attn_weights2 則為 (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "\n",
    "        # sub-layer 1: Decoder layer 自己對輸出序列做注意力。\n",
    "        # 我們同時需要 look ahead mask 以及輸出序列的 padding mask \n",
    "        # 來避免前面已生成的子詞關注到未來的子詞以及 <pad>\n",
    "        \n",
    "        attn1, attn_weights1 = self.mha1(x, x, x, combined_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "        # sub-layer 2: Decoder layer 關注 Encoder 的最後輸出\n",
    "        # 記得我們一樣需要對 Encoder 的輸出套用 padding mask 避免關注到 <pad>\n",
    "        # v向量為out1\n",
    "        \n",
    "        attn2, attn_weights2 = self.mha2(\n",
    "                                enc_output, enc_output, out1, inp_padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        \n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "        # sub-layer 3: FFN 部分跟 Encoder layer 完全一樣\n",
    "        \n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "        # 除了主要輸出 `out3` 以外，輸出 multi-head 注意權重方便之後理解模型內部狀況\n",
    "        \n",
    "        return out3, attn_weights1, attn_weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: tf.Tensor(\n",
      "[[4201   10  241   80   27    3 4202    0    0    0]\n",
      " [4201  162  467  421  189   14    7  553    3 4202]], shape=(2, 10), dtype=int64)\n",
      "--------------------\n",
      "tar_padding_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 10), dtype=float32)\n",
      "--------------------\n",
      "look_ahead_mask: tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n",
      "--------------------\n",
      "combined_mask: tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 10, 10), dtype=float32)\n",
      "(2, 1, 1, 10)\n",
      "(10, 10)\n",
      "(2, 1, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "## mask demo\n",
    "\n",
    "tar_padding_mask_demo = create_padding_mask(tar)\n",
    "look_ahead_mask_demo = create_look_ahead_mask(tar.shape[-1])\n",
    "combined_mask_demo = tf.maximum(tar_padding_mask_demo, look_ahead_mask_demo) #tf.maximum(a,b),返回的是a,b之间的最大值\n",
    "\n",
    "print(\"tar:\", tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"tar_padding_mask:\", tar_padding_mask_demo)\n",
    "print(\"-\" * 20)\n",
    "print(\"look_ahead_mask:\", look_ahead_mask_demo)\n",
    "print(\"-\" * 20)\n",
    "print(\"combined_mask:\", combined_mask_demo)\n",
    "print(tar_padding_mask_demo.shape)\n",
    "print(look_ahead_mask_demo.shape)\n",
    "print(combined_mask_demo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dec_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-e333d2417c98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# 實際初始一個 decoder layer 並做 3 個 sub-layers 的計算\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m dec_out_demo, dec_self_attn_weights_demo, dec_enc_attn_weights_demo = dec_layer(\n\u001b[0m\u001b[0;32m     32\u001b[0m     emb_tar, enc_out_demo, False, combined_mask_demo, inp_padding_mask_demo)\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dec_layer' is not defined"
     ]
    }
   ],
   "source": [
    "## decoder layer demo\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "跟 Encoder layer 相同，Decoder layer 輸出張量的最後一維也是 d_model。\n",
    "而 dec_self_attn_weights 則代表著 Decoder layer 的自注意權重，因此最後兩個維度皆為中文序列的長度 10；\n",
    "而 dec_enc_attn_weights 因為 Encoder 輸出序列的長度為 8，最後一維即爲 8。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 超參數\n",
    "\n",
    "d_model_demo = 4\n",
    "num_heads_demo = 2\n",
    "dff_demo = 8\n",
    "dec_layer_demo = DecoderLayer(d_model_demo, num_heads_demo, dff_demo)\n",
    "\n",
    "# 來源、目標語言的序列都需要 padding mask\n",
    "\n",
    "inp_padding_mask_demo = create_padding_mask(inp)\n",
    "tar_padding_mask_demo = create_padding_mask(tar)\n",
    "\n",
    "# masked MHA 用的遮罩，把 padding 跟未來子詞都蓋住\n",
    "\n",
    "look_ahead_mask_demo = create_look_ahead_mask(tar.shape[-1])\n",
    "combined_mask_demo = tf.maximum(tar_padding_mask_demo, look_ahead_mask_demo)\n",
    "\n",
    "# 實際初始一個 decoder layer 並做 3 個 sub-layers 的計算\n",
    "\n",
    "dec_out_demo, dec_self_attn_weights_demo, dec_enc_attn_weights_demo = dec_layer(\n",
    "    emb_tar, enc_out_demo, False, combined_mask_demo, inp_padding_mask_demo)\n",
    "\n",
    "print(\"emb_tar:\", emb_tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out_demo)\n",
    "print(\"-\" * 20)\n",
    "print(\"dec_out:\", dec_out_demo)\n",
    "assert emb_tar.shape == dec_out_demo.shape\n",
    "print(\"-\" * 20)\n",
    "print(\"dec_self_attn_weights.shape:\", dec_self_attn_weights_demo.shape)\n",
    "print(\"dec_enc_attn_weights:\", dec_enc_attn_weights_demo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build encoder\n",
    "\n",
    "# Encoder 的初始參數除了本來就要給 EncoderLayer 的參數還多了：\n",
    "# - num_layers: 決定要有幾個 EncoderLayers, 前面影片中的 `N`\n",
    "# - input_vocab_size: 用來把索引轉成詞嵌入向量\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "比較值得注意的是我們依照論文將 word embedding 乘上 sqrt(d_model)，\n",
    "並在 embedding 跟位置編碼相加以後通過 dropout 層來達到 regularization 的效果。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "    \n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        \n",
    "        # pos_encoding.shape == (1,input_vocab_size, d_model)\n",
    "        \n",
    "        self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\n",
    "    \n",
    "         # 建立 `num_layers` 個 EncoderLayers\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        \n",
    "        # 輸入的 x.shape == (batch_size, input_seq_len)\n",
    "        # 以下各 layer 的輸出皆為 (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        input_seq_len = tf.shape(x)[1]\n",
    "    \n",
    "        # 將 2 維的索引序列轉成 3 維的詞嵌入張量，並依照論文乘上 sqrt(d_model)\n",
    "        # 再加上對應長度的位置編碼\n",
    "        # 經過embedding後 x.shape == (batch_size, input_seq_len, d_model)\n",
    "        # input_vocab_size為pos_encoding的第一維\n",
    "        # x的第一維長度可能會比pos_encoding的第一維小，所以pos_encoding的第一維就取到input_seq_len的長度做相加\n",
    "        # pos_encoding的第零維會做broadcasting\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :input_seq_len, :]\n",
    "\n",
    "        # 對 embedding 跟位置編碼的總合做 regularization\n",
    "        # 這在 Decoder 也會做\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "    \n",
    "        # 通過 N 個 EncoderLayer 做編碼\n",
    "        \n",
    "        for i, enc_layer in enumerate(self.enc_layers):\n",
    "            x = enc_layer(x, training, mask)\n",
    "        \n",
    "        # x.shape == (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "         # 以下只是用來 demo EncoderLayer outputs\n",
    "         #print('-' * 20)\n",
    "         #print(f\"EncoderLayer {i + 1}'s output:\", x)\n",
    "      \n",
    "    \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-0be227e19ab5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# 將 2 維的索引序列丟入 Encoder 做編碼\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0menc_out_demo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"inp:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "# 超參數\n",
    "num_layers_demo = 2 # 2 層的 Encoder\n",
    "d_model_demo = 4\n",
    "num_heads_demo = 2\n",
    "dff_demo = 8\n",
    "input_vocab_size_demo = subword_encoder_en.vocab_size + 2 # 記得加上 <start>, <end>\n",
    "\n",
    "# 初始化一個 Encoder\n",
    "encoder_demo = Encoder(num_layers_demo, d_model_demo, num_heads_demo, dff_demo, input_vocab_size_demo)\n",
    "\n",
    "# 將 2 維的索引序列丟入 Encoder 做編碼\n",
    "enc_out_demo = encoder(inp, training=False, mask=None)\n",
    "print(\"inp:\", inp)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build decoder\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Decoder layer 本來就只跟 Encoder layer 差在一個 MHA，而這邏輯被包起來以後呼叫它的 Decoder 做的事情就跟 Encoder 基本上沒有兩樣了。\n",
    "在 Decoder 裡頭我們只需要建立一個專門給中文用的詞嵌入層以及位置編碼即可。\n",
    "\n",
    "\"\"\"\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \n",
    "    # 初始參數跟 Encoder 只差在用 `target_vocab_size` 而非 `inp_vocab_size`\n",
    "    \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, \n",
    "               rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        # 為中文（目標語言）建立詞嵌入層\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, self.d_model)\n",
    "        self.pos_encoding = positional_encoding(target_vocab_size, self.d_model)\n",
    "    \n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "  \n",
    "  \n",
    "    # 呼叫時的參數跟 DecoderLayer 一模一樣\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "               combined_mask, inp_padding_mask):\n",
    "    \n",
    "        tar_seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}  # 用來存放每個 Decoder layer 的注意權重\n",
    "    \n",
    "        # 這邊跟 Encoder 做的事情完全一樣\n",
    "        \n",
    "        x = self.embedding(x)  # (batch_size, tar_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :tar_seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "    \n",
    "        for i, dec_layer in enumerate(self.dec_layers):\n",
    "            x, block1, block2 = dec_layer(x, enc_output, training,\n",
    "                                    combined_mask, inp_padding_mask)\n",
    "      \n",
    "        # 將從每個 Decoder layer 取得的注意權重全部存下來回傳，方便我們觀察\n",
    "        \n",
    "            attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2\n",
    "    \n",
    "        # x.shape == (batch_size, tar_seq_len, d_model)\n",
    "        \n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: tf.Tensor(\n",
      "[[4201   10  241   80   27    3 4202    0    0    0]\n",
      " [4201  162  467  421  189   14    7  553    3 4202]], shape=(2, 10), dtype=int64)\n",
      "--------------------\n",
      "combined_mask: tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 10, 10), dtype=float32)\n",
      "--------------------\n",
      "enc_out: tf.Tensor(\n",
      "[[[-1.6643548   0.23726971  0.9922278   0.43485725]\n",
      "  [-1.6423285   0.15525073  1.0376055   0.44947207]\n",
      "  [-1.6730585   0.23806566  0.9568629   0.4781298 ]\n",
      "  [-0.8472989   1.6996098  -0.5489949  -0.3033159 ]\n",
      "  [-1.7271917   0.5071882   0.5223632   0.69764036]\n",
      "  [-1.281646   -0.36840597  1.4808921   0.16916007]\n",
      "  [-1.6862413   0.21929541  0.8610107   0.60593516]\n",
      "  [-1.6862413   0.21929541  0.8610107   0.60593516]]\n",
      "\n",
      " [[ 0.2804638   1.4288095  -0.3969432  -1.3123301 ]\n",
      "  [-1.19242     1.510546   -0.5096196   0.19149363]\n",
      "  [-1.6287227   0.40994748  0.14221759  1.0765576 ]\n",
      "  [ 0.35068548 -1.3377569   1.3880379  -0.4009665 ]\n",
      "  [-1.3767405   1.0267313  -0.52825665  0.8782658 ]\n",
      "  [-0.07331455 -1.6067501   0.86889786  0.8111668 ]\n",
      "  [-1.6858472   0.775565    0.18858123  0.72170097]\n",
      "  [-0.19478399 -1.0493888   1.6435682  -0.3993953 ]]], shape=(2, 8, 4), dtype=float32)\n",
      "--------------------\n",
      "inp_padding_mask: tf.Tensor(\n",
      "[[[[0. 0. 0. 0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0. 0. 0. 0.]]]], shape=(2, 1, 1, 8), dtype=float32)\n",
      "--------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'decoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-ca07fc1ddfa0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"inp_padding_mask:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp_padding_mask_demo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m dec_out_demo, attn_demo = decoder(tar, enc_out_demo, training=False, \n\u001b[0m\u001b[0;32m     30\u001b[0m                         \u001b[0mcombined_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcombined_mask_demo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                         inp_padding_mask=inp_padding_mask_demo)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'decoder' is not defined"
     ]
    }
   ],
   "source": [
    "## decoder demo\n",
    "\n",
    "num_layers_demo = 2 # 2 層的 Decoder\n",
    "d_model_demo = 4\n",
    "num_heads_demo = 2\n",
    "dff_demo = 8\n",
    "target_vocab_size_demo = subword_encoder_zh.vocab_size + 2 # 記得加上 <start>, <end>\n",
    "\n",
    "# 遮罩\n",
    "\n",
    "inp_padding_mask_demo = create_padding_mask(inp)\n",
    "tar_padding_mask_demo = create_padding_mask(tar)\n",
    "look_ahead_mask_demo = create_look_ahead_mask(tar.shape[1])\n",
    "combined_mask_demo = tf.math.maximum(tar_padding_mask_demo, look_ahead_mask_demo)\n",
    "\n",
    "# 初始化一個 Decoder\n",
    "\n",
    "decoder_demo = Decoder(num_layers_demo, d_model_demo, num_heads_demo, dff_demo, target_vocab_size_demo)\n",
    "\n",
    "# 將 2 維的索引序列以及遮罩丟入 Decoder\n",
    "print(\"tar:\", tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"combined_mask:\", combined_mask_demo)\n",
    "print(\"-\" * 20)\n",
    "print(\"enc_out:\", enc_out_demo)\n",
    "print(\"-\" * 20)\n",
    "print(\"inp_padding_mask:\", inp_padding_mask_demo)\n",
    "print(\"-\" * 20)\n",
    "dec_out_demo, attn_demo = decoder(tar, enc_out_demo, training=False, \n",
    "                        combined_mask=combined_mask_demo,\n",
    "                        inp_padding_mask=inp_padding_mask_demo)\n",
    "print(\"dec_out:\", dec_out_demo)\n",
    "print(\"-\" * 20)\n",
    "for block_name, attn_weights in attn_demo.items():\n",
    "    print(f\"{block_name}.shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build transformer\n",
    "\n",
    "\"\"\"\n",
    "被輸入 Transformer 的多個 2 維英文張量 inp 會一路通過 Encoder 裡頭的詞嵌入層，\n",
    "位置編碼以及 N 個 Encoder layers 後被轉換成 Encoder 輸出 enc_output，\n",
    "接著對應的中文序列 tar 則會在 Decoder 裡頭走過相似的旅程並在每一層的 Decoder layer 利用 MHA 2 關注 Encoder 的輸出 enc_output，\n",
    "最後被 Decoder 輸出。\n",
    "\n",
    "而 Decoder 的輸出 dec_output 則會通過 Final linear layer，被轉成進入 Softmax 前的 logits final_output，\n",
    "其 logit 的數目則跟中文字典裡的子詞數相同。\n",
    "\n",
    "因為 Transformer 把 Decoder 也包起來了，現在我們連 Encoder 輸出 enc_output 也不用管，\n",
    "只要把英文（來源）以及中文（目標）的索引序列 batch 丟入 Transformer，它就會輸出最後一維為中文字典大小的張量。\n",
    "第 2 維是輸出序列，裡頭每一個位置的向量就代表著該位置的中文字的機率分佈（事實上通過 softmax 才是，但這邊先這樣說方便你理解）：\n",
    "\n",
    "輸入：\n",
    "英文序列：（batch_size, inp_seq_len）\n",
    "中文序列：（batch_size, tar_seq_len）\n",
    "輸出：\n",
    "生成序列：（batch_size, tar_seq_len, target_vocab_size）\n",
    "注意權重的 dict\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Transformer 之上已經沒有其他 layers 了，我們使用 tf.keras.Model 建立一個模型\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "  # 初始參數包含 Encoder & Decoder 都需要超參數以及中英字典數目\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           input_vocab_size, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           target_vocab_size, rate)\n",
    "    \n",
    "        # 這個 FFN 輸出跟中文字典一樣大的 logits 數，等通過 softmax 就代表每個中文字的出現機率\n",
    "    \n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "  \n",
    "  # enc_padding_mask 跟 dec_padding_mask 都是英文序列的 padding mask，\n",
    "  # 只是一個給 Encoder layer 的 MHA 用，一個是給 Decoder layer 的 MHA 2 使用\n",
    "\n",
    "  def call(self, inp, tar, training, enc_padding_mask, \n",
    "           combined_mask, dec_padding_mask):\n",
    "\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    \n",
    "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    \n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, combined_mask, dec_padding_mask)\n",
    "    \n",
    "    # 將 Decoder 輸出通過最後一個 linear layer\n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: tf.Tensor(\n",
      "[[4201   10  241   80   27    3 4202    0    0    0]\n",
      " [4201  162  467  421  189   14    7  553    3 4202]], shape=(2, 10), dtype=int64)\n",
      "--------------------\n",
      "tar_inp: tf.Tensor(\n",
      "[[4201   10  241   80   27    3 4202    0    0]\n",
      " [4201  162  467  421  189   14    7  553    3]], shape=(2, 9), dtype=int64)\n",
      "--------------------\n",
      "tar_real: tf.Tensor(\n",
      "[[  10  241   80   27    3 4202    0    0    0]\n",
      " [ 162  467  421  189   14    7  553    3 4202]], shape=(2, 9), dtype=int64)\n",
      "--------------------\n",
      "predictions: tf.Tensor(\n",
      "[[[-0.03147404  0.0154118   0.00912538 ... -0.01621097  0.0621135\n",
      "   -0.07526514]\n",
      "  [-0.02995259  0.01879815  0.01268861 ...  0.04743689  0.00359119\n",
      "   -0.07843126]\n",
      "  [-0.02856873  0.01965876  0.01284878 ...  0.04336062  0.01014911\n",
      "   -0.07906348]\n",
      "  ...\n",
      "  [-0.01665978  0.00901682  0.00351396 ... -0.06069059  0.09059588\n",
      "   -0.04450906]\n",
      "  [-0.02882855  0.01974046  0.01279787 ...  0.04004341  0.01392025\n",
      "   -0.07973418]\n",
      "  [-0.03011306  0.01822157  0.01258697 ...  0.05215229 -0.00297942\n",
      "   -0.07699854]]\n",
      "\n",
      " [[-0.0315152   0.01382231  0.00807596 ... -0.02559505  0.06774879\n",
      "   -0.07195691]\n",
      "  [-0.02977074  0.01906006  0.01272305 ...  0.04519687  0.00667037\n",
      "   -0.07895765]\n",
      "  [-0.02790704  0.02028229  0.01273092 ...  0.0325062   0.02332664\n",
      "   -0.08032081]\n",
      "  ...\n",
      "  [-0.0158558   0.00679437  0.00219178 ... -0.06737265  0.09155638\n",
      "   -0.03853924]\n",
      "  [-0.02853991  0.01996272  0.0127809  ...  0.03705865  0.01767302\n",
      "   -0.08007555]\n",
      "  [-0.02884655  0.01957314  0.01281917 ...  0.04302017  0.01030187\n",
      "   -0.07920419]]], shape=(2, 9, 4203), dtype=float32)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "decoder_layer1_block1 (2, 2, 9, 9)\n",
      "decoder_layer1_block2 (2, 2, 9, 8)\n"
     ]
    }
   ],
   "source": [
    "## transformer demo\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "在序列生成任務裡頭，模型獲得的正確答案是輸入序列往左位移一個位置的結果。\n",
    "你現在應該明白 Transformer 在訓練的時候並不是吃整個中文序列，\n",
    "而是吃一個去掉尾巴的序列 tar_inp，然後試著去預測「左移」一個字以後的序列 tar_real。\n",
    "同樣概念當然也可以運用到以 RNN 或是 CNN-based 的模型上面。\n",
    "\n",
    "從影片中你也可以發現給定 tar_inp 序列中的任一位置，其對應位置的 tar_real 就是下個時間點模型應該要預測的中文字。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 超參數\n",
    "\n",
    "num_layers_demo = 1\n",
    "d_model_demo = 4\n",
    "num_heads_demo = 2\n",
    "dff_demo = 8\n",
    "\n",
    "# + 2 是為了 <start> & <end> token\n",
    "\n",
    "input_vocab_size_demo = subword_encoder_en.vocab_size + 2\n",
    "output_vocab_size_demo = subword_encoder_zh.vocab_size + 2\n",
    "\n",
    "# 重點中的重點。訓練時用前一個字來預測下一個中文字\n",
    "\n",
    "tar_inp_demo = tar[:, :-1]\n",
    "tar_real_demo = tar[:, 1:]\n",
    "\n",
    "# 來源 / 目標語言用的遮罩。注意 `comined_mask` 已經將目標語言的兩種遮罩合而為一\n",
    "\n",
    "inp_padding_mask_demo = create_padding_mask(inp)\n",
    "tar_padding_mask_demo = create_padding_mask(tar_inp_demo)\n",
    "look_ahead_mask_demo = create_look_ahead_mask(tar_inp_demo.shape[1])\n",
    "combined_mask_demo = tf.math.maximum(tar_padding_mask_demo, look_ahead_mask_demo)\n",
    "\n",
    "# 初始化我們的第一個 transformer\n",
    "\n",
    "transformer_demo = Transformer(num_layers_demo, d_model_demo, num_heads_demo, dff_demo, \n",
    "                          input_vocab_size_demo, output_vocab_size_demo)\n",
    "\n",
    "# 將英文、中文序列丟入取得 Transformer 預測下個中文字的結果\n",
    "\n",
    "predictions_demo, attn_weights_demo = transformer_demo(inp, tar_inp_demo, False, inp_padding_mask_demo, \n",
    "                                        combined_mask_demo, inp_padding_mask_demo)\n",
    "\n",
    "print(\"tar:\", tar)\n",
    "print(\"-\" * 20)\n",
    "print(\"tar_inp:\", tar_inp_demo)\n",
    "print(\"-\" * 20)\n",
    "print(\"tar_real:\", tar_real_demo)\n",
    "print(\"-\" * 20)\n",
    "print(\"predictions:\", predictions_demo)\n",
    "print('-'*100)\n",
    "for key in attn_weights_demo:\n",
    "    print(key,attn_weights_demo[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 模型訓練過程\n",
    "\n",
    "# 1.initialize model\n",
    "# 2.define loss optimizer, define learning rate schedule\n",
    "# 3.train step 單步的訓練\n",
    "# 4.train process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate = (d_model ** -0.5) * min(step_num ** -0.5, step_num *warm_up ** -1.5)\n",
    "# learning rate 先增後減\n",
    "\n",
    "class learning_rate_schedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    # 論文預設 `warmup_steps` = 4000\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(learning_rate_schedule, self).__init__()\n",
    "    \n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        \n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2) # tf.math.rsqrt(x),x的開根號分之一\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d_models = [128, 256, 512]\n",
    "warmup_steps = [1000 * i for i in range(1, 4)]\n",
    "\n",
    "schedules = []\n",
    "labels = []\n",
    "colors = [\"blue\", \"red\", \"black\"]\n",
    "for d in d_models:\n",
    "    schedules += [learning_rate_schedule(d, s) for s in warmup_steps]\n",
    "    labels += [f\"d_model: {d}, warm: {s}\" for s in warmup_steps]\n",
    "\n",
    "for i, (schedule, label) in enumerate(zip(schedules, labels)):\n",
    "    plt.plot(schedule(tf.range(10000, dtype=tf.float32)), \n",
    "           label=label, color=colors[i // 3])\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "我們在這邊將 reduction 參數設為 none，請 loss_object 不要把每個位置的 error 加總。\n",
    "而這是因為我們之後要自己把 <pad> token 出現的位置的損失捨棄不計。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "             from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \n",
    "    # 這次的 mask 將序列中不等於 0 的位置視為 1，其餘為 0 \n",
    "    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    \n",
    "    # 照樣計算所有位置的 cross entropy 但不加總\n",
    "    \n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask  # 只計算非 <pad> 位置的損失 \n",
    "    \n",
    "  \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the (weighted) mean of the given values.\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "# 我们使用 tf.keras.metrics.SparseCategoricalAccuracy.result() 方法输出最终的评估指标值（预测正确的样本数占总样本数的比例）。\n",
    "\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "               name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "在實際訓練 Transformer 之前還需要定義一個簡單函式來產生所有的遮罩：\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 為 Transformer 的 Encoder / Decoder 準備遮罩\n",
    "\n",
    "def create_masks(inp, tar):\n",
    "    \n",
    "    # 英文句子的 padding mask，要交給 Encoder layer 自注意力機制用的\n",
    "    \n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "    # 同樣也是英文句子的 padding mask，但是是要交給 Decoder layer 的 MHA 2 \n",
    "    # 關注 Encoder 輸出序列用的\n",
    "    \n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "    # Decoder layer 的 MHA1 在做自注意力機制用的\n",
    "    # `combined_mask` 是中文句子的 padding mask 跟 look ahead mask 的疊加\n",
    "    \n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_vocab_size: 8137\n",
      "target_vocab_size: 4203\n",
      "這個 Transformer 有 4 層 Encoder / Decoder layers\n",
      "d_model: 128\n",
      "num_heads: 8\n",
      "dff: 512\n",
      "input_vocab_size: 8137\n",
      "target_vocab_size: 4203\n",
      "dropout_rate: 0.1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## create object of model\n",
    "\n",
    "num_layers = 4 \n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = subword_encoder_en.vocab_size + 2\n",
    "target_vocab_size = subword_encoder_zh.vocab_size + 2\n",
    "dropout_rate = 0.1  # 預設值\n",
    "\n",
    "print(\"input_vocab_size:\", input_vocab_size)\n",
    "print(\"target_vocab_size:\", target_vocab_size)\n",
    "\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, dropout_rate)\n",
    "\n",
    "print(f\"\"\"這個 Transformer 有 {num_layers} 層 Encoder / Decoder layers\n",
    "d_model: {d_model}\n",
    "num_heads: {num_heads}\n",
    "dff: {dff}\n",
    "input_vocab_size: {input_vocab_size}\n",
    "target_vocab_size: {target_vocab_size}\n",
    "dropout_rate: {dropout_rate}\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將客製化 learning rate schdeule 丟入 Adam opt.\n",
    "\n",
    "# Adam opt. 的參數都跟論文相同\n",
    "\n",
    "learning_rate = learning_rate_schedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沒找到 checkpoint，從頭訓練。\n"
     ]
    }
   ],
   "source": [
    "# 方便比較不同實驗/ 不同超參數設定的結果\n",
    "\n",
    "run_id = f\"{num_layers}layers_{d_model}d_{num_heads}heads_{dff}dff_{train_part}train_part\"\n",
    "checkpoint_path = os.path.join(checkpoint_path, run_id)\n",
    "log_dir = os.path.join(log_dir, run_id)\n",
    "\n",
    "# tf.train.Checkpoint 可以幫我們把想要存下來的東西整合起來，方便儲存與讀取\n",
    "# 一般來說你會想存下模型以及 optimizer 的狀態\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "# ckpt_manager 會去 checkpoint_path 看有沒有符合 ckpt 裡頭定義的東西\n",
    "# 存檔的時候只保留最近 5 次 checkpoints，其他自動刪除\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# 如果在 checkpoint 路徑上有發現檔案就讀進來\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  \n",
    "    # 用來確認之前訓練多少 epochs 了\n",
    "    last_epoch = int(ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "    print(f'已讀取最新的 checkpoint，模型已訓練 {last_epoch} epochs。')\n",
    "else:\n",
    "    last_epoch = 0\n",
    "    print(\"沒找到 checkpoint，從頭訓練。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Coding\\\\Projects\\\\transformer\\\\Data file\\\\logs\\\\4layers_128d_8heads_512dff_20train_part'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Coding\\\\Projects\\\\transformer\\\\Data file\\\\checkpoints\\\\4layers_128d_8heads_512dff_20train_part'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "如果你曾經以TensorFlow 2 實作過稍微複雜一點的模型，應該就知道 train_step 函式的寫法非常固定：\n",
    "\n",
    "對輸入數據做些前處理（本文中的遮罩、將輸出序列左移當成正解 etc.）\n",
    "利用 tf.GradientTape 輕鬆記錄數據被模型做的所有轉換並計算 loss\n",
    "將梯度取出並讓 optimzier 對可被訓練的權重做梯度下降（上升）\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "@tf.function  # 讓 TensorFlow 幫我們將 eager code 優化並加快運算\n",
    "\n",
    "def train_step(inp, tar):\n",
    "    # 前面說過的，用去尾的原始序列去預測下一個字的序列\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "  \n",
    "    # 建立 3 個遮罩\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "    # 紀錄 Transformer 的所有運算過程以方便之後做梯度下降\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 注意是丟入 `tar_inp` 而非 `tar`。記得將 `training` 參數設定為 True\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "        # 跟影片中顯示的相同，計算左移一個字的序列跟模型預測分佈之間的差異，當作 loss\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    # 取出梯度並呼叫前面定義的 Adam optimizer 幫我們更新 Transformer 裡頭可訓練的參數\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "    # 將 loss 以及訓練 acc 記錄到 TensorBoard 上，非必要\n",
    "    \n",
    "    train_loss(loss) # 將loss傳入後變成累計的平均值\n",
    "    train_accuracy(tar_real, predictions) # 累計的accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "此超參數組合的 Transformer 已經訓練 0 epochs。\n",
      "剩餘 epochs：-20\n",
      "Saving checkpoint for epoch 1 at E:\\Coding\\Projects\\transformer\\Data file\\checkpoints\\4layers_128d_8heads_512dff_20train_part\\ckpt-1\n",
      "Epoch 1 Loss 5.1789 Accuracy 0.0222\n",
      "Time taken for 1 epoch: 88.90571737289429 secs\n",
      "\n",
      "Saving checkpoint for epoch 2 at E:\\Coding\\Projects\\transformer\\Data file\\checkpoints\\4layers_128d_8heads_512dff_20train_part\\ckpt-2\n",
      "Epoch 2 Loss 4.2413 Accuracy 0.0587\n",
      "Time taken for 1 epoch: 35.161297082901 secs\n",
      "\n",
      "Saving checkpoint for epoch 3 at E:\\Coding\\Projects\\transformer\\Data file\\checkpoints\\4layers_128d_8heads_512dff_20train_part\\ckpt-3\n",
      "Epoch 3 Loss 3.7265 Accuracy 0.1009\n",
      "Time taken for 1 epoch: 24.2880117893219 secs\n",
      "\n",
      "Saving checkpoint for epoch 4 at E:\\Coding\\Projects\\transformer\\Data file\\checkpoints\\4layers_128d_8heads_512dff_20train_part\\ckpt-4\n",
      "Epoch 4 Loss 3.2533 Accuracy 0.1538\n",
      "Time taken for 1 epoch: 30.766900300979614 secs\n",
      "\n",
      "Saving checkpoint for epoch 5 at E:\\Coding\\Projects\\transformer\\Data file\\checkpoints\\4layers_128d_8heads_512dff_20train_part\\ckpt-5\n",
      "Epoch 5 Loss 2.9576 Accuracy 0.1828\n",
      "Time taken for 1 epoch: 24.302238941192627 secs\n",
      "\n",
      "Saving checkpoint for epoch 6 at E:\\Coding\\Projects\\transformer\\Data file\\checkpoints\\4layers_128d_8heads_512dff_20train_part\\ckpt-6\n",
      "Epoch 6 Loss 2.7701 Accuracy 0.1995\n",
      "Time taken for 1 epoch: 19.575437545776367 secs\n",
      "\n",
      "Saving checkpoint for epoch 7 at E:\\Coding\\Projects\\transformer\\Data file\\checkpoints\\4layers_128d_8heads_512dff_20train_part\\ckpt-7\n",
      "Epoch 7 Loss 2.6305 Accuracy 0.2129\n",
      "Time taken for 1 epoch: 19.49204707145691 secs\n",
      "\n",
      "Saving checkpoint for epoch 8 at E:\\Coding\\Projects\\transformer\\Data file\\checkpoints\\4layers_128d_8heads_512dff_20train_part\\ckpt-8\n",
      "Epoch 8 Loss 2.5131 Accuracy 0.2251\n",
      "Time taken for 1 epoch: 24.318838834762573 secs\n",
      "\n",
      "Saving checkpoint for epoch 9 at E:\\Coding\\Projects\\transformer\\Data file\\checkpoints\\4layers_128d_8heads_512dff_20train_part\\ckpt-9\n",
      "Epoch 9 Loss 2.4004 Accuracy 0.2379\n",
      "Time taken for 1 epoch: 19.62043786048889 secs\n",
      "\n",
      "Saving checkpoint for epoch 10 at E:\\Coding\\Projects\\transformer\\Data file\\checkpoints\\4layers_128d_8heads_512dff_20train_part\\ckpt-10\n",
      "Epoch 10 Loss 2.2904 Accuracy 0.2511\n",
      "Time taken for 1 epoch: 24.32796335220337 secs\n",
      "\n",
      "Saving checkpoint for epoch 11 at E:\\Coding\\Projects\\transformer\\Data file\\checkpoints\\4layers_128d_8heads_512dff_20train_part\\ckpt-11\n",
      "Epoch 11 Loss 2.1860 Accuracy 0.2639\n",
      "Time taken for 1 epoch: 19.50938367843628 secs\n",
      "\n",
      "Saving checkpoint for epoch 12 at E:\\Coding\\Projects\\transformer\\Data file\\checkpoints\\4layers_128d_8heads_512dff_20train_part\\ckpt-12\n",
      "Epoch 12 Loss 2.0871 Accuracy 0.2769\n",
      "Time taken for 1 epoch: 24.317797422409058 secs\n",
      "\n",
      "Saving checkpoint for epoch 13 at E:\\Coding\\Projects\\transformer\\Data file\\checkpoints\\4layers_128d_8heads_512dff_20train_part\\ckpt-13\n",
      "Epoch 13 Loss 1.9969 Accuracy 0.2893\n",
      "Time taken for 1 epoch: 19.648147344589233 secs\n",
      "\n",
      "Saving checkpoint for epoch 14 at E:\\Coding\\Projects\\transformer\\Data file\\checkpoints\\4layers_128d_8heads_512dff_20train_part\\ckpt-14\n",
      "Epoch 14 Loss 1.9138 Accuracy 0.3007\n",
      "Time taken for 1 epoch: 19.60990858078003 secs\n",
      "\n",
      "Saving checkpoint for epoch 15 at E:\\Coding\\Projects\\transformer\\Data file\\checkpoints\\4layers_128d_8heads_512dff_20train_part\\ckpt-15\n",
      "Epoch 15 Loss 1.8380 Accuracy 0.3111\n",
      "Time taken for 1 epoch: 26.679381847381592 secs\n",
      "\n",
      "Saving checkpoint for epoch 16 at E:\\Coding\\Projects\\transformer\\Data file\\checkpoints\\4layers_128d_8heads_512dff_20train_part\\ckpt-16\n",
      "Epoch 16 Loss 1.7690 Accuracy 0.3206\n",
      "Time taken for 1 epoch: 19.479853630065918 secs\n",
      "\n",
      "Saving checkpoint for epoch 17 at E:\\Coding\\Projects\\transformer\\Data file\\checkpoints\\4layers_128d_8heads_512dff_20train_part\\ckpt-17\n",
      "Epoch 17 Loss 1.7044 Accuracy 0.3298\n",
      "Time taken for 1 epoch: 19.701555490493774 secs\n",
      "\n",
      "Saving checkpoint for epoch 18 at E:\\Coding\\Projects\\transformer\\Data file\\checkpoints\\4layers_128d_8heads_512dff_20train_part\\ckpt-18\n",
      "Epoch 18 Loss 1.6454 Accuracy 0.3383\n",
      "Time taken for 1 epoch: 19.59164547920227 secs\n",
      "\n",
      "Saving checkpoint for epoch 19 at E:\\Coding\\Projects\\transformer\\Data file\\checkpoints\\4layers_128d_8heads_512dff_20train_part\\ckpt-19\n",
      "Epoch 19 Loss 1.5739 Accuracy 0.3487\n",
      "Time taken for 1 epoch: 24.526458978652954 secs\n",
      "\n",
      "Saving checkpoint for epoch 20 at E:\\Coding\\Projects\\transformer\\Data file\\checkpoints\\4layers_128d_8heads_512dff_20train_part\\ckpt-20\n",
      "Epoch 20 Loss 1.5099 Accuracy 0.3579\n",
      "Time taken for 1 epoch: 24.51102590560913 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 定義我們要看幾遍數據集\n",
    "EPOCHS = 20\n",
    "print(f\"此超參數組合的 Transformer 已經訓練 {last_epoch} epochs。\")\n",
    "print(f\"剩餘 epochs：{min(0, last_epoch - EPOCHS)}\")\n",
    "\n",
    "\n",
    "# 用來寫資訊到 TensorBoard，非必要但十分推薦\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# 比對設定的 `EPOCHS` 以及已訓練的 `last_epoch` 來決定還要訓練多少 epochs\n",
    "for epoch in range(0, EPOCHS):\n",
    "    \n",
    "    start = time.time()\n",
    "  \n",
    "    # 重置紀錄 TensorBoard 的 metrics\n",
    "    \n",
    "    train_loss.reset_states() # 當前的epoch累積的loss & accuracy\n",
    "    train_accuracy.reset_states()\n",
    "  \n",
    "    # 一個 epoch 就是把我們定義的訓練資料集一個一個 batch 拿出來處理，直到看完整個數據集 \n",
    "    \n",
    "    for (step_idx, (inp, tar)) in enumerate(train_dataset):\n",
    "    \n",
    "        # 每次 step 就是將數據丟入 Transformer，讓它生預測結果並計算梯度最小化 loss\n",
    "        train_step(inp, tar)  \n",
    "\n",
    "    # 每個 epoch 完成就存一次檔  \n",
    "    \n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "    \n",
    "    # 將 loss 以及 accuracy 寫到 TensorBoard 上\n",
    "    \n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar(\"train_loss\", train_loss.result(), step=epoch + 1)\n",
    "        tf.summary.scalar(\"train_acc\", train_accuracy.result(), step=epoch + 1)\n",
    "  \n",
    "    print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,  # epoch+1 為了讓epoch從1開始打印\n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "    print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 15452), started 0:02:14 ago. (Use '!kill 15452' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-72975d1aaf08e816\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-72975d1aaf08e816\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {logdir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "eval : A B C D ->E\n",
    "       A B C D E ->F\n",
    "       \n",
    "       \n",
    "跟訓練的時候不同，在做預測時我們不需做 teacher forcing 來穩定 Transformer 的訓練過程。\n",
    "反之，我們將 Transformer 在每個時間點生成的中文索引加到之前已經生成的序列尾巴，並以此新序列作為其下一次的輸入。\n",
    "這是因為 Transformer 事實上是一個自迴歸模型（Auto-regressive model）：依據自己生成的結果預測下次輸出。\n",
    "\n",
    "利用 Transformer 進行翻譯（預測）的邏輯如下：\n",
    "\n",
    "將輸入的英文句子利用 Subword Tokenizer 轉換成子詞索引序列（還記得 inp 吧？）\n",
    "在該英文索引序列前後加上代表英文 BOS / EOS 的 tokens\n",
    "在 Transformer 輸出序列長度達到 MAX_LENGTH 之前重複以下步驟：\n",
    "為目前已經生成的中文索引序列產生新的遮罩\n",
    "將剛剛的英文序列、當前的中文序列以及各種遮罩放入 Transformer\n",
    "將 Transformer 輸出序列的最後一個位置的向量取出，並取 argmax 取得新的預測中文索引\n",
    "將此索引加到目前的中文索引序列裡頭作為 Transformer 到此為止的輸出結果\n",
    "如果新生成的中文索引為 <end> 則代表中文翻譯已全部生成完畢，直接回傳\n",
    "將最後得到的中文索引序列回傳作為翻譯結果       \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 給定一個英文句子，輸出預測的中文索引數字序列以及注意權重 dict\n",
    "def evaluate(inp_sentence):\n",
    "  \n",
    "    # 準備英文句子前後會加上的 <start>, <end>\n",
    "    start_token = [subword_encoder_en.vocab_size]\n",
    "    end_token = [subword_encoder_en.vocab_size + 1]\n",
    "  \n",
    "    # inp_sentence 是字串，我們用 Subword Tokenizer 將其變成子詞的索引序列\n",
    "    # 並在前後加上 BOS / EOS\n",
    "    inp_sentence = start_token + subword_encoder_en.encode(inp_sentence) + end_token\n",
    "    \n",
    "    # encoder_input.shape == (1,inp_seq_len)\n",
    "    \n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0) #加入batch維度\n",
    "  \n",
    "    # 跟我們在影片裡看到的一樣，Decoder 在第一個時間點吃進去的輸入\n",
    "    # 是一個只包含一個中文 <start> token 的序列\n",
    "    \n",
    "    # decoder_input.shape == (1,1)\n",
    "    decoder_input = [subword_encoder_zh.vocab_size]\n",
    "    \n",
    "    output = tf.expand_dims(decoder_input, 0)  # 增加 batch 維度\n",
    "  \n",
    "    # auto-regressive，一次生成一個中文字並將預測加到輸入再度餵進 Transformer\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        \n",
    "        # 每多一個生成的字就得產生新的遮罩\n",
    "        # 此時look_ahead_mask其實沒用因為現在的decoder_inp是沒有後面的單詞的\n",
    "        \n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "                            encoder_input, output)\n",
    "  \n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "    \n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                    output,\n",
    "                                                    False,\n",
    "                                                    enc_padding_mask,\n",
    "                                                    combined_mask,\n",
    "                                                    dec_padding_mask)\n",
    "    \n",
    "\n",
    "        # 將序列中最後一個 distribution 取出，並將裡頭值最大的當作模型最新的預測字\n",
    "        \n",
    "        predictions = predictions[: , -1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "        # 遇到 <end> token 就停止回傳，代表模型已經產生完結果\n",
    "        \n",
    "        if tf.equal(predicted_id, subword_encoder_zh.vocab_size + 1):\n",
    "            \n",
    "            return tf.squeeze(output, axis=0), attention_weights # tf.squeeze 維度壓縮\n",
    "    \n",
    "        #將 Transformer 新預測的中文索引加到輸出序列中，讓 Decoder 可以在產生\n",
    "        # 下個中文字的時候關注到最新的 `predicted_id`\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "        # 將 batch 的維度去掉後回傳預測的中文索引序列\n",
    "        \n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: America, Taiwan and France.\n",
      "--------------------\n",
      "predicted_seq: tf.Tensor([4201   30    4   37  630 1094    8   57    4    3], shape=(10,), dtype=int32)\n",
      "--------------------\n",
      "predicted_sentence: 美国、台湾和法国。\n"
     ]
    }
   ],
   "source": [
    "# 要被翻譯的英文句子\n",
    "\n",
    "sentence = \"America, Taiwan and France.\"\n",
    "\n",
    "# 取得預測的中文索引序列\n",
    "predicted_seq, _ = evaluate(sentence)\n",
    "\n",
    "# 過濾掉 <start> & <end> tokens 並用中文的 subword tokenizer 幫我們將索引序列還原回中文句子\n",
    "target_vocab_size = subword_encoder_zh.vocab_size\n",
    "predicted_seq_without_bos_eos = [idx for idx in predicted_seq if idx < target_vocab_size]\n",
    "predicted_sentence = subword_encoder_zh.decode(predicted_seq_without_bos_eos)\n",
    "\n",
    "print(\"sentence:\", sentence)\n",
    "print(\"-\" * 20)\n",
    "print(\"predicted_seq:\", predicted_seq)\n",
    "print(\"-\" * 20)\n",
    "print(\"predicted_sentence:\", predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_4 (Encoder)          multiple                  1834624   \n",
      "_________________________________________________________________\n",
      "decoder_4 (Decoder)          multiple                  1596288   \n",
      "_________________________________________________________________\n",
      "dense_175 (Dense)            multiple                  542187    \n",
      "=================================================================\n",
      "Total params: 3,973,099\n",
      "Trainable params: 3,973,099\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
